{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054f2c6a",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">CQF - FINAL EXAM </h1>\n",
    "<h1 style=\"text-align: center;\">Portfolio Construction using Black-Litterman Model and Factors </h1>\n",
    "<h3 style=\"text-align: center;\">Michele Vannucci</h3>\n",
    "<h3 style=\"text-align: center;\">January 2025 Cohort</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b6793",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "This notebook presents the core code structure accompanied by essential technical comments. For clarity and readability, all plots and printed outputs are omitted here, as well as theoretical explanations of the framework; they are included and discussed in detail in the accompanying PDF report.\n",
    "Only technical aspects are reported in this document, such as function design, algorithms descriptions, data manipulation procedures, and output formats. The notebook is designed to run autonomously. Some of the required data files are retrieved directly through the code, while others were downloaded manually. All manually obtained files will be included in the same zip archive as this notebook and are assumed to be stored in a `Data/` directory located in the notebook’s root folder.\n",
    "The Python libraries used are listed together with their versions, and the entire project was executed in a virtual environment running Python 3.10.16.\n",
    "\n",
    "**Notation**\n",
    "- Dates: reported as `YY-MM-DD`.\n",
    "- Data frequency: computations use weekly returns; all results are reported at an annualized frequency.\n",
    "- Vectors and matrices are denoted in boldface, while the transposition operator is indicated with a prime symbol ($'$). The identity matrix is written as $\\mathbb{I}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import time\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np                          # v 2.1.3\n",
    "import pandas as pd                         # v 2.2.3\n",
    "import matplotlib.pyplot as plt             # v 3.10.1\n",
    "import seaborn as sns                       # v 0.13.2\n",
    "\n",
    "# Data retrieval\n",
    "import yfinance as yf                       # v 0.2.61\n",
    "\n",
    "# Statistical modeling\n",
    "import statsmodels.api as sm                # v 0.14.4\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Optimization\n",
    "from scipy.optimize import minimize, \\\n",
    "                           minimize_scalar, \\\n",
    "                           LinearConstraint # v 1.15.2\n",
    "from sklearn.covariance import LedoitWolf   # v 1.6.1\n",
    "from cvxopt import matrix, solvers          # v 1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177ce2c",
   "metadata": {},
   "source": [
    "# Data retrieval and preprocessing\n",
    "The workflow follows this structure:\n",
    "1. **Data acquisition** – downloading required datasets, either manually or programmatically.\n",
    "2. **Index alignment** – ensuring consistent time periods across all dataframes and defining start and end dates.\n",
    "3. **Resampling** – converting all series to weekly frequency, using Friday as the reference date.\n",
    "4. **Computation of excess returns** – calculating returns in excess of the risk-free rate.\n",
    "  \n",
    "The following datasets have beeen manually downloaded, as they are not retrieved automatically by the notebook:\n",
    "- **Fama-French 5 factors**   \n",
    "Available at https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html  \n",
    "Section U.S. *Research Returns Data* → *Fama/French 5 Factors (2x3) [Daily]*  \n",
    "Saved as: `FF5_Factors_daily.csv`\n",
    "- **Momentum Factor**  \n",
    "Available at the same link, under *Sorts involving Prior Returns* → *Momentum Factor (Mom) [Daily]*  \n",
    "Saved as: `FF_Mom_daily.csv`\n",
    "- **Betting Against Beta (BAB)**  \n",
    "Downloaded from https://www.aqr.com/Insights/Datasets/Betting-Against-Beta-Equity-Factors-Daily (daily data)  \n",
    "The file, originally provided in `.xlsx` format, is converted to `.csv` and contains multiple countries. Only the USA column is used.  \n",
    "Saved as: `BAB_AQR_daily.csv`\n",
    "- **3-Month Treasury Bill Rate**  \n",
    "Available at https://fred.stlouisfed.org/series/TB3MS (monthly data)  \n",
    "Saved as: `TB3MS.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ad966",
   "metadata": {},
   "source": [
    "## Data acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36657e8f",
   "metadata": {},
   "source": [
    "The list below contains all tickers selected for the analysis. `factor_proxy_tickers` stores the ETFs considered as potential proxies for the factors, while `asset_tickers` contains the remaining portfolio assets. `spy_ticker` is used as the benchmark. Daily adjusted close prices for all ETF tickers are saved as `.csv` files in the `Data/` folder for offline processing. All available historical data are retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a71fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_ticker = \"SPY\"   \n",
    "\n",
    "factor_proxy_tickers = [\"IJR\",\"VB\",\"SLYV\",\"SCHA\",\"IWM\",       # SMB\n",
    "                        \"VLUE\",\"SPVU\",\"VTV\",\"IVE\",\"IVLU\",     # HML\n",
    "                        \"SPMO\",\"QMOM\",\"MTUM\",                 # MOM\n",
    "                        \"QUAL\",                               # RMW\n",
    "                        \"SYLD\",                               # CMA\n",
    "                        \"SPLV\", \"SPHB\"                        # BAB\n",
    "                        ]     \n",
    "\n",
    "asset_tickers = [\n",
    "                 \"QQQ\",                           # growth\n",
    "                 \"SPHQ\",                          # quality  \n",
    "                 \"XLE\",\"XLP\",\"XLV\",\"XLF\",\"XLU\",   # sector rotation\n",
    "                 \"TLT\",\"IEF\",                     # rates and term structure\n",
    "                 \"LQD\",\"HYG\",                     # credit spread risk\n",
    "                 \"TIP\",                           # inflation beta\n",
    "                 \"VNQ\",                           # real estate\n",
    "                 \"GLD\",                           # gold\n",
    "                 \"DBC\",                           # commodities\n",
    "                 \"VIXM\",                          # volatility\n",
    "                 ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_requests = {\n",
    "    \"benchmark\": {\n",
    "        \"tickers\": spy_ticker, \n",
    "        \"filename\": \"Data/spy_daily.csv\"\n",
    "    },\n",
    "    \"factors\": {\n",
    "        \"tickers\": factor_proxy_tickers, \n",
    "        \"filename\": \"Data/factor_proxy_daily.csv\"\n",
    "    },\n",
    "    \"assets\":{\n",
    "        \"tickers\": asset_tickers,\n",
    "        \"filename\": \"Data/asset_prices_daily.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, spec in data_requests.items():\n",
    "    tickers = spec[\"tickers\"]\n",
    "    tickers = [tickers] if isinstance(tickers, str) else tickers\n",
    "\n",
    "    df = yf.download(\n",
    "        tickers,\n",
    "        interval=\"1d\",\n",
    "        auto_adjust=True,\n",
    "        progress=False\n",
    "    )[\"Close\"]\n",
    "\n",
    "    df.to_csv(spec[\"filename\"])\n",
    "    print(f\"Saved {name} data to {spec['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965613cf",
   "metadata": {},
   "source": [
    "## Index and dates alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_daily_close = pd.read_csv(\"Data/factor_proxy_daily.csv\", index_col=\"Date\", parse_dates=True)\n",
    "FF5_daily_ret     = pd.read_csv(\"Data/FF5_Factors_daily.csv\",  index_col=\"Date\", parse_dates=True) \n",
    "Mom_daily_ret     = pd.read_csv(\"Data/FF_Mom_daily.csv\",       index_col=\"Date\", parse_dates=True) \n",
    "BAB_daily_ret     = pd.read_csv(\"Data/BAB_AQR_daily.csv\",      index_col=\"Date\", parse_dates=True) \n",
    "rf3m_monthly      = pd.read_csv('Data/TB3MS.csv',              index_col=\"Date\", parse_dates=True) \n",
    "spy_daily_close   = pd.read_csv('Data/spy_daily.csv',          index_col=\"Date\", parse_dates=True) \n",
    "asset_daily_close = pd.read_csv(\"Data/asset_prices_daily.csv\", index_col=\"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d34e1",
   "metadata": {},
   "source": [
    "It is essential that all data frames share the same time window and have a consistent index. The initial and final dates are identified by checking the first and last valid observations across all series. The youngest (latest-starting) ETF determines the sample start, while the series with the earliest termination determines the sample end. Daily prices are later converted to weekly frequency by taking the Friday close, so the time grid is aligned to Fridays. The sample ends on 2025-04-25, the last Friday on or before the earliest final date among all series, corresponding to the Betting Against Beta factor. Because the inaugural week (Wednesday 2015-12-02 to Friday 2015-12-04) lacks a prior Friday close, its return is undefined and automatically discarded, making the week ending 2015-12-11 the first valid observation. This truncated two-day span at the start of the sample is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e39d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([proxy_daily_close, FF5_daily_ret, Mom_daily_ret, BAB_daily_ret], axis=1)\n",
    "\n",
    "# First and last available dates for each column\n",
    "first_dates = df_all.apply(lambda col: col.first_valid_index())\n",
    "last_dates  = df_all.apply(lambda col: col.last_valid_index())\n",
    "\n",
    "# Youngest (latest starting) series\n",
    "youngest_ticker = first_dates.idxmax()\n",
    "youngest_date   = first_dates.max()\n",
    "\n",
    "# Oldest (earliest ending) series\n",
    "oldest_end_ticker = last_dates.idxmin()\n",
    "oldest_end_date   = last_dates.min()\n",
    "\n",
    "start_date = pd.Timestamp(year=youngest_date.year,\n",
    "                          month=youngest_date.month,\n",
    "                          day=1)\n",
    "\n",
    "# Find last available Friday \n",
    "end_date = oldest_end_date - pd.Timedelta(days=(oldest_end_date.weekday() - 4) % 7)\n",
    "\n",
    "print(f\"Start date:   {start_date}\")\n",
    "print(f\"End date:     {end_date}\")\n",
    "print(f\"Youngest ETF: {youngest_ticker} ({youngest_date})\")\n",
    "print(f\"Earliest end: {oldest_end_ticker} ({end_date})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_daily_close = proxy_daily_close.loc[start_date : end_date]\n",
    "FF5_daily_ret     = FF5_daily_ret    .loc[start_date : end_date]\n",
    "Mom_daily_ret     = Mom_daily_ret    .loc[start_date : end_date]\n",
    "BAB_daily_ret     = BAB_daily_ret    .loc[start_date : end_date]\n",
    "rf3m_monthly      = rf3m_monthly     .loc[start_date : end_date]\n",
    "spy_daily_close   = spy_daily_close  .loc[start_date : end_date]\n",
    "asset_daily_close = asset_daily_close.loc[start_date : end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b05de8",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2d8a8",
   "metadata": {},
   "source": [
    "Factors and Treasurey Bill data are expressed as percent returns, so they need to be scaled back to decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "FF5_daily_ret = FF5_daily_ret/100.0\n",
    "Mom_daily_ret = Mom_daily_ret/100.0\n",
    "BAB_daily_ret = BAB_daily_ret/100.0\n",
    "rf3m_monthly  = rf3m_monthly/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d8bf9",
   "metadata": {},
   "source": [
    "The Fama-French file reports the market excess return as `Mkt-RF`, where the risk-free leg is the one-month Treasury bill (`RF`). Because this study adopts the three-month bill as its risk-free asset, `Mkt-RF` must first be turned back into a total market return by adding `RF`; the excess return relative to the three-month bill will then obtained by subtracting the three-month yield.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba724fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "FF5_daily_ret['MKT'] = FF5_daily_ret['Mkt-RF'] + FF5_daily_ret['RF']\n",
    "FF5_daily_ret.drop(['Mkt-RF','RF'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca2f68",
   "metadata": {},
   "source": [
    "Each dataset is converted to a coherent Friday-to-Friday panel before excess returns are formed. Weeks in which U.S. markets are closed on the Friday contain no daily observation, but the for the weekly resampling Thursday data is used, thanks to `.last()`. An explicit example is Good Friday, 2022-04-15, which is filled using data from 2022-04-14 data.\n",
    "- Daily closing prices are resampled to Friday observations with `resample(\"W-FRI\").last()`, after which one-week simple returns are obtained via `pct_change()`: $$r^{\\text{week}}_t = \\dfrac{P_t}{P_{t-1}} - 1$$\n",
    "- Daily Fama-French factor values are already expressed as daily simple returns, so the corresponding weekly return is derived by compounding within each week:\n",
    "  $$r^{\\text{week}}_t \\;=\\; \\prod_{d\\in\\text{week}}\\bigl(1 + r_d\\bigr) \\;-\\; 1$$\n",
    "- The three–month Treasury-bill series is supplied as a monthly annualized yield. First, each observation is converted into a one-week simple return: $$r^{\\text{week}}_t \\;=\\; \\bigl(1 + y^{\\text{ann}}_t\\bigr)^{1/52} - 1$$ \n",
    "Then it is resampled so that each Friday inside a given month receives the most recently published monthly yield. Then the series is extended up to the end of April using `.reindex(full_fridays)`, where `full_fridays` is the grid of all Fridays during the time window considered; these slots are finally forward filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_weekly_ret = proxy_daily_close.resample(\"W-FRI\").last().pct_change()\n",
    "spy_weekly_ret   = spy_daily_close  .resample(\"W-FRI\").last().pct_change()\n",
    "asset_weekly_ret = asset_daily_close.resample(\"W-FRI\").last().pct_change()\n",
    "\n",
    "FF5_weekly_ret = (1 + FF5_daily_ret).resample(\"W-FRI\").prod() - 1\n",
    "Mom_weekly_ret = (1 + Mom_daily_ret).resample(\"W-FRI\").prod() - 1\n",
    "BAB_weekly_ret = (1 + BAB_daily_ret).resample(\"W-FRI\").prod() - 1\n",
    "\n",
    "WEEKS_PER_YEAR = 52\n",
    "full_fridays = pd.date_range(start_date, end_date, freq=\"W-FRI\")\n",
    "rf3m_weekly = (1 + rf3m_monthly).pow(1/WEEKS_PER_YEAR) - 1      \n",
    "\n",
    "rf3m_weekly = (rf3m_weekly\n",
    "             .resample(\"W-FRI\").ffill()\n",
    "             .reindex(full_fridays)\n",
    "             .ffill()\n",
    "            )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping together all factor data\n",
    "factors_weekly_ret = pd.concat([FF5_weekly_ret, Mom_weekly_ret, BAB_weekly_ret], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da13a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ETF proxy:    ', proxy_weekly_ret.shape)\n",
    "print('FF factors:   ', factors_weekly_ret.shape)\n",
    "print('TB3MS:        ', rf3m_weekly.shape)\n",
    "print('SPY benchmark:', spy_weekly_ret.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f6ed2",
   "metadata": {},
   "source": [
    "Excess returns are obtained by subtracting the three-month Treasury-bill rate from the corresponding simple returns. Because the Fama–French factor series are already defined net of the risk-free rate, this adjustment is applied solely to the market return and to each ETF proxy; the factor series themselves remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_exc = factors_weekly_ret.copy()\n",
    "factors_exc['MKT'] = factors_weekly_ret['MKT'].subtract(rf3m_weekly['TB3MS'], axis=0)\n",
    "\n",
    "proxy_exc = proxy_weekly_ret.subtract(rf3m_weekly['TB3MS'], axis=0)\n",
    "spy_exc   = spy_weekly_ret  .subtract(rf3m_weekly['TB3MS'], axis=0)\n",
    "asset_exc = asset_weekly_ret.subtract(rf3m_weekly['TB3MS'], axis=0)\n",
    "\n",
    "proxy_exc   = proxy_exc.dropna()\n",
    "spy_exc     = spy_exc.dropna()\n",
    "factors_exc = factors_exc.dropna()\n",
    "asset_exc   = asset_exc.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26786117",
   "metadata": {},
   "source": [
    "Building the *Bet-Against-Beta* custom factor proxy as the difference of excess returns of $\\text{SPLV}$ and $\\text{SPHB}$: $$\\text{BAB} = r_{\\text{SPLV}} - r_{\\text{SPHB}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0615e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_exc['BAB_p'] = proxy_exc['SPLV'] - proxy_exc['SPHB'] # _p stands for 'proxy' to avoid naming confusion\n",
    "proxy_exc.drop(['SPLV','SPHB'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd1b0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31742fec",
   "metadata": {},
   "source": [
    "# Factor selection\n",
    "The objective is to identify suitable ETF proxies for the Fama–French factors. To this end, an OLS regression including all factors is estimated for each candidate. The best proxies are selected according to the following guidelines:\n",
    "- **Factor sensitivity**: $\\beta > 0.8$ ensures a strong sensitivity to the intended factor.\n",
    "- **Cross-factor neutrality**: $|\\beta| < 0.3$ limits unintended exposures to other factors.\n",
    "- **Market exposure**: $\\beta \\in [0.8,1.2]$ confirms appropriate sensitivity to the benchmark.\n",
    "- **No unexplained drift**: $\\alpha \\approx 0$ indicates the absence of persistent unexplained performance.\n",
    "- **Model fit**: $R^{2} > 0.8$ requires that at least 80\\% of return variability is explained by the factor model.\n",
    "\n",
    "These thresholds are indicative rather than absolute; in practice, some may be relaxed if justified.\n",
    "\n",
    "The weekly excess‐return of each ETF is regressed on the set of the 5 Fama-French factors plus the Momentum and Betting Against Beta: $\\text{factors} = \\{\\text{MKT, SMB, HML, RMW, CMA, MOM, BAB}\\}$ by ordinary least squares(OLS): $$r_t^{\\text{ETF}}=\\alpha + \\sum_{k \\,\\in\\, \\text{factors}} \\beta_{k_t} \\cdot \\,k_t + \\varepsilon_t$$\n",
    "Here $\\alpha$ captures any systematic drift or tracking-error that is not explained by the factors, while $\\varepsilon_t$ represents the remaining idiosyncratic shock and is assumed to have zero mean. \n",
    "\n",
    "To obtain heteroskedasticity- and autocorrelation-consistent inference, Newey–West (HAC) standard errors are computed with four lags. Four weekly lags capture one calendar month of serial correlation, which is sufficient for assets whose memory rarely exceeds that horizon. The HAC adjustment leaves the point estimates $\\alpha$ and $\\beta_k$ unchanged but replaces the usual OLS standard errors with robust ones, allowing valid t-statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe785aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proxy_test(\n",
    "        etf, target_factor,\n",
    "        proxy_exc, factors_exc,\n",
    "        nw_lags=4\n",
    "):\n",
    "    \"\"\"\n",
    "    OLS + Newey-West factor regression.\n",
    "    proxy_exc: DataFrame of ETF excess returns (proxies)\n",
    "    factors_exc: DataFrame of factor excess returns\n",
    "    \"\"\"\n",
    "    \n",
    "    factors = ['MKT','SMB','HML','MOM','RMW','CMA','BAB']\n",
    "    \n",
    "    def _nw_fit(y, X):\n",
    "        ols = sm.OLS(y, X).fit()\n",
    "        nw  = ols.get_robustcov_results(cov_type=\"HAC\", maxlags=nw_lags)\n",
    "        p   = pd.Series(nw.params,  index=ols.params.index)\n",
    "        t   = pd.Series(nw.tvalues, index=ols.params.index)\n",
    "        return p, t, ols.rsquared\n",
    "    \n",
    "    # Align and merge the ETF and factors\n",
    "    y = proxy_exc[etf]\n",
    "    X = factors_exc[factors]\n",
    "    y, X = y.align(X, join=\"inner\", axis=0)\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    beta, tstat, r2 = _nw_fit(y, X)\n",
    "    \n",
    "    cross = {\n",
    "        f: beta[f] for f in factors if f != target_factor\n",
    "    }\n",
    "\n",
    "    # Results\n",
    "    print(f\"\\n=== {etf} vs {target_factor} ===\")\n",
    "    print(f\"β_{target_factor}: {beta[target_factor]:.2f}, \"\n",
    "          f\"NW t-stat: {tstat[target_factor]:.2f}\")\n",
    "    if cross:\n",
    "        print(\"Factor loadings:\",\n",
    "              {k: f\"{v:.2f}\" for k, v in cross.items()})\n",
    "    print(f\"α (weekly): {beta['const']:.4f} \"\n",
    "          f\"→ ≈ {beta['const']*WEEKS_PER_YEAR:.2%} p.a., \"\n",
    "          f\"t-stat: {tstat['const']:.2f}\")\n",
    "    print(f\"R² = {r2:.2f}\")\n",
    "    return beta, tstat, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51471dbd",
   "metadata": {},
   "source": [
    "The regression is performed only for meaningful proxy–factor pairs, rather than for every possible combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_groups = {\n",
    "    \"SMB\": [\"IJR\",\"IWM\",\"VB\",\"SCHA\",\"SLYV\"],\n",
    "    \"HML\": [\"VLUE\",\"IVLU\",\"VTV\",\"IVE\",\"SPVU\"],\n",
    "    \"MOM\": [\"MTUM\",\"QMOM\",\"SPMO\"],\n",
    "    \"RMW\": [\"QUAL\"],\n",
    "    \"CMA\": [\"SYLD\"],                \n",
    "    \"BAB\": [\"BAB_p\"]           \n",
    "}\n",
    "\n",
    "for factor, etfs in proxy_groups.items():\n",
    "    for etf in etfs:\n",
    "        run_proxy_test(\n",
    "            etf=etf,\n",
    "            target_factor=factor,\n",
    "            proxy_exc=proxy_exc,\n",
    "            factors_exc=factors_exc,\n",
    "            nw_lags=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5cf1d",
   "metadata": {},
   "source": [
    "A detailed analysis of the regression results is provided in the report. The final set of ETFs selected as factor proxies are: $\\text{IWM}$ for $\\text{SMB}$, $\\text{SPVU}$ for $\\text{HML}$, $\\text{MTUM}$ for $\\text{MOM}$, and $\\text{BAB_p}$ for $\\text{BAB}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512dc6a",
   "metadata": {},
   "source": [
    "Checking if $\\alpha_{\\text{IWM}}$ is economically insignificant with respect to annual volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "iwm_vol_annual = proxy_exc['IWM'].std(ddof=1) * np.sqrt(52)\n",
    "print(f\"IWM annual volatility: {(iwm_vol_annual*100).round(2)}% p.a.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172c22c",
   "metadata": {},
   "source": [
    "## P&L Analysis\n",
    "Comparing the cumulative performance of the selected factor proxies against the benchmark ($\\text{SPY}$). Weekly excess returns are aggregated, and cumulative returns are computed as: $$ \\text{CumRet}_t = \\prod_{s=1}^t (1+r_s)-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ac10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = ['IWM','SPVU','MTUM','BAB_p']\n",
    "final_factors_exc = proxy_exc[facts]\n",
    "final_factors_exc = final_factors_exc.dropna()\n",
    "\n",
    "final_exc = pd.concat([final_factors_exc, spy_exc], axis=1)\n",
    "cum_ret = (1 + final_exc).cumprod() - 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cum_ret.plot(ax=ax, linewidth=1.4)                     \n",
    "ax.set_title(\"Cumulative Return — Factors vs SPY\", fontsize=14, weight='bold')\n",
    "ax.set_ylabel(\"Cumulative return\")\n",
    "ax.set_xlabel(\"\")                                     \n",
    "ax.axhline(0, color=\"grey\", lw=0.8, linestyle=\"--\")   \n",
    "ax.legend(ncol=3, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0621b",
   "metadata": {},
   "source": [
    "## Rolling quantities plots\n",
    "Rolling $\\alpha$ and $\\beta$ estimates are computed over a two-year moving window. While $\\beta$ is dimentionless, $\\alpha$  represents a weekly return and is therefore annualized for interpretation. The definitions adopted are: $$\\beta = \\frac{\\text{Cov}(r_{asset}, f)}{\\text{Var}(f)}$$ $$\\alpha = \\bar{r}_{asset} - \\beta \\,\\bar{f}$$ where bars denote sample means over the chosen time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pairs = {\n",
    "    \"IWM   vs SMB\":  (\"IWM\",  \"SMB\"),\n",
    "    \"SPVU  vs HML\": (\"SPVU\", \"HML\"),\n",
    "    \"MTUM  vs MOM\": (\"MTUM\", \"MOM\"),\n",
    "    \"BAB_p vs BAB\":(\"BAB_p\",\"BAB\") \n",
    "}\n",
    "\n",
    "window = 104  # ≈ 2 years\n",
    "\n",
    "for title, (etf, factor) in selected_pairs.items():\n",
    "\n",
    "    y = proxy_exc[etf]\n",
    "    x = factors_exc[factor]\n",
    "    data = pd.concat([y, x], axis=1).dropna()        \n",
    "    y, x = data.iloc[:, 0], data.iloc[:, 1]          \n",
    "\n",
    "    rolling_cov  = y.rolling(window).cov(x)\n",
    "    rolling_var  = x.rolling(window).var()\n",
    "    rolling_beta = rolling_cov / rolling_var\n",
    "\n",
    "    mean_y  = y.rolling(window).mean()\n",
    "    mean_x  = x.rolling(window).mean()\n",
    "    rolling_alpha = (mean_y - rolling_beta * mean_x) * WEEKS_PER_YEAR \n",
    "\n",
    "    # plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 4))\n",
    "    ax1.plot(rolling_beta.index, rolling_beta, label=\"Beta\", color=\"tab:blue\")\n",
    "    ax1.set_ylabel(\"β\", color=\"tab:blue\")\n",
    "    ax1.tick_params(axis='y', labelcolor=\"tab:blue\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(rolling_alpha.index, rolling_alpha,\n",
    "             label=\"Alpha (annualised)\", color=\"tab:red\", linestyle=\"--\")\n",
    "    ax2.set_ylabel(\"α  (annual %)\", color=\"tab:red\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"tab:red\")\n",
    "\n",
    "    ax1.set_title(f\"Rolling {window}-Week β and α: {title}\")\n",
    "    ax1.axhline(0, lw=0.7, ls=\":\", color=\"tab:blue\") # zero-beta\n",
    "    ax2.axhline(0, lw=0.7, ls=\":\", color=\"tab:red\") # zero-alpha\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d96b3",
   "metadata": {},
   "source": [
    "## Performance Statistics \n",
    "\n",
    "For each asset or factor return series, the following annualized performance metrics are computed from weekly excess returns:\n",
    "\n",
    "- **Annual Return**: Geometric mean return scaled to an annual horizon, $$\\text{Ann. Ret} = \\left[ \\prod_{t=1}^{T} (1 + r_t) \\right]^{\\frac{52}{T}} - 1$$ \n",
    "\n",
    "- **Annual Volatility**: Standard deviation of weekly returns multiplied by the square root of the annualization factor, $$\\text{Ann. Vol} = \\sigma_{\\text{weekly}} \\times \\sqrt{52}$$ using the sample standard deviation (`ddof=1`).\n",
    "\n",
    "- **Sharpe Ratio**: Risk-adjusted return using the ratio of annual return to annual volatility, $$\\text{Sharpe} = \\frac{\\text{Ann. Ret}}{\\text{Ann. Vol}}$$ (since the returns are in excess form, no risk-free subtraction is required).\n",
    "\n",
    "- **Maximum Drawdown**: Largest peak-to-trough decline in the cumulative return curve, $$\\text{Max DD} = \\min_{t} \\left( \\frac{V_t}{\\max_{s \\leq t} V_s} - 1 \\right)$$ where $V_t = \\text{CumRet}_t + 1$ is the cumulative return index.\n",
    "\n",
    "These metrics summarise both absolute and risk-adjusted performance, allowing for a direct comparison of factor proxies and the benchmark over the sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for col in final_exc.columns:\n",
    "    r = final_exc[col]\n",
    "    \n",
    "    ann_ret = (1 + r).prod()**(WEEKS_PER_YEAR / len(r)) - 1\n",
    "    ann_vol = r.std(ddof=1) * np.sqrt(WEEKS_PER_YEAR)  \n",
    "    sharpe  = ann_ret / ann_vol\n",
    "    dd_curve = (1 + r).cumprod()\n",
    "    max_dd  = ((dd_curve / dd_curve.cummax()) - 1).min()\n",
    "\n",
    "    results[col] = [ann_ret, ann_vol, sharpe, max_dd]\n",
    "\n",
    "stats_df = pd.DataFrame.from_dict(\n",
    "    results, \n",
    "    orient='index', \n",
    "    columns=['Annual Ret', 'Annual Vol', 'Sharpe', 'Max DD']\n",
    ")\n",
    "\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d92b8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95458d60",
   "metadata": {},
   "source": [
    "# Correlation Structure and Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a0726",
   "metadata": {},
   "source": [
    "Once the asset universe is defined, the first step is to analyse the correlation and covariance matrices. This stage is crucial, as all subsequent results will be heavily influenced by these estimates. Moreover, a poorly specified covariance matrix can lead to unstable results and even cause algorithmic failures, particularly in operations requiring matrix inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4967ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_universe_exc = pd.concat([final_factors_exc, asset_exc], axis=1)\n",
    "tickers = asset_universe_exc.columns\n",
    "\n",
    "corr_matrix = asset_universe_exc.corr()\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c114e6",
   "metadata": {},
   "source": [
    "Because the sample spans 2015 – 2025, it straddles the COVID-19 shock—a period that may have altered the dependence structure among ETFs.\n",
    "Before estimating the full-sample covariance matrix, the data are therefore split at 28 Feb 2020. For each segment the pair-wise correlation matrix is calculated and its grand mean reported; the difference matrix $\\Delta\\boldsymbol{\\rho}=\\boldsymbol{\\rho}_{post} - \\boldsymbol{\\rho}_{pre}$ is then visualised as a heat-map to highlight where correlations strengthened (red) or weakened (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = \"2020-02-28\"\n",
    "pre  = asset_universe_exc.loc[:cut]\n",
    "post = asset_universe_exc.loc[cut:]\n",
    "\n",
    "corr_pre  = pre.corr()\n",
    "corr_post = post.corr()\n",
    "\n",
    "avg_pre  = corr_pre.values[np.triu_indices(20, k=1)].mean()\n",
    "avg_post = corr_post.values[np.triu_indices(20, k=1)].mean()\n",
    "print(f\"Pre-COVID avg ρ = {avg_pre:.2f}, post-COVID avg ρ = {avg_post:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_diff = corr_post - corr_pre\n",
    "\n",
    "# redefine scale limits for better image clarity\n",
    "band = 0.65        \n",
    "vmin, vmax = -band, band\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "im = plt.imshow(corr_diff, cmap=\"coolwarm\", vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04,\n",
    "             label=\"Δ Correlation (Post - Pre)\")\n",
    "plt.xticks(ticks=np.arange(len(tickers)), labels=tickers, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(tickers)), labels=tickers)\n",
    "plt.title(\"Change in Pairwise Correlations\\n(Post-COVID - Pre-COVID)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77779917",
   "metadata": {},
   "source": [
    "An average correlation jump from 0.20 → 0.30 around COVID is non-trivial and a static $\\boldsymbol{\\Sigma}$ estimated over the full 2015–2025 span would blend two very different regimes. For this reason, an exponentially weighted covariance (EWMA) is employed.\n",
    "The exponentially weighted mean is updated recursively, $$\\boldsymbol{\\mu}_t = \\alpha \\boldsymbol{r}_t + (1-\\alpha)\\boldsymbol{\\mu}_{t-1}$$ and the corresponding covariance matrix is $$\\boldsymbol{\\Sigma}_t = \\alpha (\\boldsymbol{r}_t-\\boldsymbol{\\mu}_t)(\\boldsymbol{r}_t-\\boldsymbol{\\mu}_t)'+(1-\\alpha)\\boldsymbol{\\Sigma}_{t-1}$$  \n",
    "The smoothing parameter $\\alpha$ controls the emphasis placed on recent observations: values closer to 1 assign greater weight to the most recent regime, which is particularly relevant for capturing the structural break introduced by the COVID-19 period. In practice, it is often expressed in terms of the decay factor $\\lambda = 1 - \\alpha$, which specifies the weight assigned to the previous covariance estimate. Following the RiskMetrics (2001) guidelines, $\\lambda=0.94$ is suggested for daily data. To adapt this to weekly data, the decay factor is rescaled as $\\lambda_{\\text{weekly}} = \\lambda_{\\text{daily}}^5$, assuming five trading days per week. The smoothing parameter is then: $$\\alpha = 1 - \\lambda_{weekly}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20906c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2661 # 1 - (0.94)^5\n",
    "cov_ewma = asset_universe_exc.ewm(alpha=alpha, adjust=False).cov()\n",
    "Sigma_df = cov_ewma.loc[end_date]\n",
    "Sigma = Sigma_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58036e08",
   "metadata": {},
   "source": [
    "Before proceeding, the condition number of the covariance matrix $\\boldsymbol{\\Sigma}$ is assessed by computing the ratio between its smallest and largest eigenvalues. If this ratio is exceedingly small (i.e., $\\ll 10^{-5}$), then $\\boldsymbol{\\Sigma}$ is deemed nearly singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84639759",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = np.linalg.eigvalsh(Sigma)    \n",
    "print(\"Smallest eigenvalue:\", eigs.min())\n",
    "print(\"Largest eigenvalue:\", eigs.max())\n",
    "print(\"Ratio λ_min/λ_max:\", eigs.min()/eigs.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d31b6",
   "metadata": {},
   "source": [
    "The ratio between the largest and smallest eigenvalues is on the order of $10^{-7}$, indicating that the covariance matrix is numerically ill-conditioned.  \n",
    "To regularise the estimate, a Ledoit–Wolf shrinkage estimator is applied instead: $$\\boldsymbol{\\Sigma}_{\\text{LW}} = (1-\\delta)\\boldsymbol{\\Sigma}+\\delta \\frac{\\text{Tr}(\\boldsymbol{\\Sigma})}{N} \\mathbb{I}_N$$ where $N$ is the dimensionality of the matrix, $\\mathbb{I}_N$ is the ($N \\times N$) identity matrix, $\\text{Tr}(\\cdot)$ denotes the trace operator and $\\delta$ is the shrinkage intensity chosen to minimise the mean-squared error between $\\boldsymbol{\\Sigma}_{\\text{LW}}$ and the (unknown) true covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = asset_universe_exc.values    \n",
    "lw = LedoitWolf(store_precision=True).fit(returns)\n",
    "Sigma_lw  = lw.covariance_      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36911ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = np.linalg.eigvalsh(Sigma_lw)    \n",
    "print(\"Smallest eigenvalue:\", eigs.min())\n",
    "print(\"Largest eigenvalue:\", eigs.max())\n",
    "print(\"Ratio λ_min/λ_max:\", eigs.min()/eigs.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb60b20",
   "metadata": {},
   "source": [
    "A clear improvement is achieved, with the eigenvalue ratio increasing to the order of $10^{-3}$. The shrunk covariance matrix will therefore be used in all subsequent operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76925a9c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ea845",
   "metadata": {},
   "source": [
    "# Black–Litterman Model\n",
    "\n",
    "The implementation proceeds in four stages.\n",
    "1. The equilibrium excess-return vector $\\boldsymbol{\\pi}$ is estimated after computing $\\mathbf{w}_{\\text{mkt}}$ and $\\lambda_{\\text{mkt}}$.\n",
    "2. Definition of the views and their uncertainties. \n",
    "3. The view-uncertainty matrix $\\boldsymbol{\\Omega}$ is built. \n",
    "4. Posterior return and optimal weights \n",
    "\n",
    "A table precedes each stage to list the variables introduced up to that point and to record their dimensions, as matrix operations can become hard to follow.\n",
    "Throughout, a column vector is written $(M\\times1)$ and a row vector $(1\\times M)$.\n",
    "Consequently, a row–by–column product produces a scalar $(1\\times1)$, whereas a column–by–row product produces an outer-product matrix $(M\\times M)$.  \n",
    "Variables appear in every table in the exact order in which they are generated by the accompanying code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4997c",
   "metadata": {},
   "source": [
    "## Part I - Equilibrium Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872f888",
   "metadata": {},
   "source": [
    "<div style=\"margin:20px 0; width:500px;\">\n",
    "  <table style=\"border-collapse:collapse; font-family:Arial, sans-serif; font-size:14px;\">\n",
    "    <colgroup>\n",
    "      <col style=\"width:100px; text-align:center;\">\n",
    "      <col style=\"width:90px; text-align:center;\">\n",
    "      <col style=\"width:280px; text-align:left;\">\n",
    "    </colgroup>\n",
    "    <thead>\n",
    "      <tr style=\"background-color:#f2f2f2;\">\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Variable</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Shape</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Description</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{w}_{\\text{mkt}}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Market Weights</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\lambda_{\\text{mkt}}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$1 \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Market Risk Aversion coefficient</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\boldsymbol{\\Sigma}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times N$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Covariance Matrix (Ledoit–Wolf shrunk)</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\boldsymbol{\\pi}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Implied Equilibrium Return Vector</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bc754",
   "metadata": {},
   "source": [
    "### Market Weights\n",
    "Market weights are used to construct the prior return vector. Five asset classes are defined: Equities, Fixed Income, Real Assets, Factors, and Volatility. Each portfolio constituent is assigned to one of these categories, as asset classes typically have distinct allocation weights in global portfolios. The partition reported by State Street Investment Management in Global Market Portfolio (GMP) 2024 serves as the reference. Details on the GMP composition and weight computation are provided in the accompanying PDF report.  \n",
    "\n",
    "For each asset in the portfolio, market capitalization is calculated as: $$\\text{Market Cap} = \\text{Shares Outstanding} \\times \\text{Price}$$ If this information is unavailable in Yahoo Finance, Assets Under Management (AUM) are used as an approximation.  \n",
    "\n",
    "The Factors and Volatility categories are explicitly assigned zero weight, as these exposures are synthetic constructs rather than directly investable securities with a market capitalization. Finally, within each basket, individual asset weights are determined proportionally to their market capitalizations, and each basket is normalised to sum to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f59f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_classes = {\n",
    "    \"Equities\":     [\"QQQ\",\"SPHQ\",\"XLE\",\"XLF\",\"XLP\",\"XLU\",\"XLV\"],\n",
    "    \"Fixed Income\": [\"LQD\",\"HYG\",\"IEF\",\"TLT\",\"TIP\"],\n",
    "    \"Real Assets\":  [\"GLD\",\"VNQ\",\"DBC\"],\n",
    "    \"Factors\":      [\"IWM\",\"SPVU\",\"MTUM\",\"BAB_p\"],\n",
    "    \"Volatility\":   [\"VIXM\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market capitalizations expressed in Trillion USD\n",
    "GMP_alloc = {\n",
    "    \"Equities\": 78.0,\n",
    "    \"Fixed Income\": 57.23,\n",
    "    \"Real Assets\": 12.95\n",
    "}\n",
    "\n",
    "# computing asset classes weights\n",
    "total_included = sum(GMP_alloc.values())\n",
    "GMP_percent = {k: v / total_included for k, v in GMP_alloc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d76f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = {\"IWM\", \"SPVU\", \"MTUM\", \"BAB_p\", \"VIXM\"}\n",
    "subset_tickers = [t for t in asset_universe_exc.columns if t not in exclude]\n",
    "\n",
    "rows = []\n",
    "for tic in subset_tickers:\n",
    "    tk = yf.Ticker(tic)\n",
    "    info = tk.info or {}\n",
    "    \n",
    "    # Try market cap from sharesOutstanding × price\n",
    "    mc = None\n",
    "    shares = info.get(\"sharesOutstanding\")\n",
    "    if shares:\n",
    "        # use price last date of the dataset\n",
    "        hist = tk.history(start=end_date, end=pd.to_datetime(end_date) + pd.Timedelta(days=1))\n",
    "        if not hist.empty:\n",
    "            mc = shares * hist[\"Close\"].iloc[0]\n",
    "    \n",
    "    # Use totalAssets (AUM) if mc is None or NaN\n",
    "    if mc is None or (isinstance(mc, float) and np.isnan(mc)):\n",
    "        mc = info.get(\"totalAssets\", None)\n",
    "    \n",
    "    rows.append({\"Asset\": tic, \"MarketCap_or_AUM\": mc})\n",
    "\n",
    "df_marketcap = pd.DataFrame(rows)\n",
    "market_caps = df_marketcap.set_index(\"Asset\")[\"MarketCap_or_AUM\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for cls, tickers_cls in asset_classes.items():  \n",
    "    # Total market cap within each class\n",
    "    class_cap_total = sum(\n",
    "        mc for t in tickers_cls\n",
    "        if (mc := market_caps.get(t)) is not None and mc > 0\n",
    "    )\n",
    "    # External weight (GMP)\n",
    "    class_ext_weight = GMP_percent.get(cls, 0.0)  # 0 for Factors/Volatility\n",
    "\n",
    "    for t in tickers_cls:\n",
    "        cap = market_caps.get(t, None)\n",
    "        if cap and cap > 0 and class_cap_total > 0 and class_ext_weight > 0:\n",
    "            internal_w = cap / class_cap_total\n",
    "            prior_w = class_ext_weight * internal_w\n",
    "        else:\n",
    "            internal_w = 0.0\n",
    "            prior_w = 0.0\n",
    "        rows.append({\n",
    "            \"Ticker\": t,\n",
    "            \"AssetClass\": cls,\n",
    "            \"MarketCap_or_AUM\": cap,\n",
    "            \"InternalWeight\": internal_w,\n",
    "            \"ClassWeight\": class_ext_weight,\n",
    "            \"PriorWeight\": prior_w\n",
    "        })\n",
    "\n",
    "df_prior = pd.DataFrame(rows).set_index(\"Ticker\")\n",
    "print(df_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prior = df_prior.reindex(tickers)\n",
    "w_mkt = df_prior[\"PriorWeight\"].to_numpy()\n",
    "\n",
    "# checking weights normalization\n",
    "print(w_mkt.sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d50e5d",
   "metadata": {},
   "source": [
    "### Market risk-aversion coefficient\n",
    "The market risk-aversion coefficient $\\lambda_{\\mathbf{mkt}}$ is derived from the unconstrained maximization of the mean–variance utility problem $$U(\\mathbf{w})=\\mathbf{w}'\\boldsymbol{\\mu} - \\lambda \\mathbf{w}'\\boldsymbol{\\Sigma} \\mathbf{w}$$ where $\\mathbf{w}$ is the vector of risky-asset weights, $\\boldsymbol{\\mu}$ the vector of excess expected returns, and $\\boldsymbol{\\Sigma}$ the return-covariance matrix. The first order condition leads to $$\\boldsymbol{\\mu} = 2 \\lambda \\boldsymbol{\\Sigma} \\mathbf{w}$$ \n",
    "Left-multiplying both sides by $\\mathbf{w}'$ and defining $$\\sigma_{\\text{mkt}}^2 := \\mathbf{w}' \\boldsymbol{\\Sigma} \\mathbf{w} \\;\\;\\; \\text{and} \\;\\;\\; \\mu_{\\text{mkt}} := \\mathbf{w}'\\boldsymbol{\\mu}$$ yields the market risk aversion coefficient: $$\\hat{\\lambda}_{\\text{mkt}}=\\frac{\\hat{\\mu}_\\text{mkt}}{\\hat{\\sigma}_\\text{mkt}^2}$$\n",
    "In practice, these quantities are estimated from weekly excess returns of $\\text{SPY}$, taken as a proxy for the U.S. market. The estimators are $$\\hat{\\mu}_{\\text{mkt}} = \\frac{1}{T}\\sum_{t=1}^{T}(r_t - r_{f,t}) \\;\\;\\;\\;\\hat{\\sigma}_{\\text{mkt}}^{2}= \\frac{1}{T-1}\\sum_{t=1}^{T} \\Bigl[(r_t - r_{f,t}) - \\hat{\\mu}_{\\text{mkt}}\\Bigr]^{2}$$ where $r_t$ denotes the weekly total return of $\\text{SPY}$, $r_{f,t}$ the weekly risk-free rate, and $T$ the number of observations. The corresponding estimate of the market risk aversion is $$\\lambda_{\\text{mkt}}=\\frac{\\mu_\\text{mkt}}{\\sigma_\\text{mkt}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mkt = spy_exc.mean()\n",
    "var_mkt = spy_exc.var(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mkt = mu_mkt / (2*var_mkt)\n",
    "print(lambda_mkt.round(4))\n",
    "lambda_mkt = lambda_mkt.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77c0be",
   "metadata": {},
   "source": [
    "### Equlibrium return\n",
    "The equilibrium return is obtained by solving the reverse optimisation problem. In the classical mean–variance framework, the optimization seeks portfolio weights given an expected return vector; here, the process is inverted, and the expected return vector is inferred from a given set of market weights. In formula:\n",
    "$$\\boldsymbol{\\pi} = 2 \\lambda_{\\text{mkt}}\\boldsymbol{\\Sigma} \\mathbf{w}_{\\text{mkt}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87885bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = 2 * lambda_mkt * (Sigma_lw @ w_mkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b3ea8",
   "metadata": {},
   "source": [
    "## Part II - Views Definition\n",
    "<div style=\"float:left; margin:20px 0; width:500px;\">\n",
    "  <table style=\"border-collapse:collapse; font-family:Arial, sans-serif; font-size:14px;\">\n",
    "    <colgroup>\n",
    "      <col style=\"width:100px; text-align:center;\">\n",
    "      <col style=\"width:90px; text-align:center;\">\n",
    "      <col style=\"width:280px; text-align:left;\">\n",
    "    </colgroup>\n",
    "    <thead>\n",
    "      <tr style=\"background-color:#f2f2f2;\">\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Variable</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Shape</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Description</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{P}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$K \\times N$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Pick Matrix</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{Q}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$K \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">View Vector</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{C}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$K \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Views Confidence Level Vector</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331c7d9",
   "metadata": {},
   "source": [
    "\n",
    "The pick matrix $P$ is the mathematical object that directly links the views to the corresponding assets. Each row matches a view: only the assets affected by that have a non-zero entry, and the value will be different depending on the type of view. This will become more clear after defining the views themselves and their features. The $\\mathbf{Q}$ and $\\mathbf{C}$ vectors respectively store the views and their confidence levels.\n",
    "\n",
    "Here is the list of the views:\n",
    "- **View 1**: *Gold will deliver a +7.0% excess return over the risk-free rate over the next year*. (Confidence level 60%)\n",
    "- **View 2**: *TIPS ETF will outperform cash, reflecting a rising real yields by 2% in the next 12 months*. (Confidence level 45%)\n",
    "- **View 3**: *7–10 yr Treasuries is expected to underperform High-yield credit by 5% in the next year*. (Confidence level 55%)\n",
    "- **View 4**: *The US large-cap growth and real estate will outperform IG credit by 1.2% in the next year*. (Confidence level 50%)\n",
    "- **View 5**: *Financial and Healthcare cyclicals are expected to outperform by 3.5% the basket of Energy, Staples & Utilities cyclicals in the next year*. (Confidence level 65%)\n",
    "\n",
    "The pick matrix $\\mathbf{P}$ is generated from an intial draft in which each view stores: (i) the tickers involved, (ii) their signs ($+1$ for the outperforming set, $–1$ for the underperforming set), and (iii) a boolean flag indicating whether the view is absolute or relative.\n",
    "- Absolute views: place a $+1$ on the target asset (zeros elsewhere).\n",
    "- Relative views: replace the $\\pm 1$ indicators with market weights separated by sign. This approach is motivated by the fact that applying equal weights would impose disproportionately large adjustments on smaller-cap assets.\n",
    "\n",
    "Consequently, each relative‑view row in $\\mathbf{P}$ sums to zero (long basket minus short basket), while absolute‑view rows retain a single non‑zero entry. This implementation makes the views management clean, and it is easily scalable for larger portfolios or the inclusion of more views.  \n",
    " \n",
    "It should be noted that the direction of the tilt induced by a view cannot be inferred from the sign of the view alone. In the absence of constraints and additional views, if the magnitude of the view is smaller than the difference between the implied equilibrium returns of two assets, the model tilts the portfolio toward the asset with the lower implied equilibrium return; if the view’s magnitude exceeds that difference, the portfolio is tilted toward the asset with the higher implied equilibrium return.\n",
    "Another critical aspect is that each view is expressed on an annual basis, while the datasets have been resampled weekly. To align these frequencies, every entry in the $\\mathbf{Q}$ vector is divided by 52, the number of weeks in a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c14d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the views dictionary \n",
    "views = {\n",
    "    'V1': {\n",
    "        'legs':   {'GLD': +1},\n",
    "        'relative': False\n",
    "    },\n",
    "    'V2': {\n",
    "        'legs':   {'TIP': +1},\n",
    "        'relative': False\n",
    "    },\n",
    "    'V3': {\n",
    "        'legs':   {'HYG': +1, 'IEF': -1},\n",
    "        'relative': True\n",
    "    },\n",
    "    'V4': {\n",
    "        'legs':   {'QQQ': +1, 'VNQ': +1, 'LQD': -1},\n",
    "        'relative': True\n",
    "    },\n",
    "    'V5': {\n",
    "        'legs':   {'XLF': +1, 'XLV': +1, 'XLE': -1, 'XLP': -1, 'XLU': -1},\n",
    "        'relative': True\n",
    "    }\n",
    "}\n",
    "\n",
    "P_raw = pd.DataFrame(0.0, index=views.keys(), columns=tickers)\n",
    "is_relative = pd.Series({name: props['relative'] for name, props in views.items()})\n",
    "\n",
    "for name, props in views.items():\n",
    "    for ticker, sign in props['legs'].items():\n",
    "        P_raw.at[name, ticker] = sign\n",
    "\n",
    "print(\"Skeleton P_raw:\")\n",
    "print(P_raw)\n",
    "print(\"\\nIs each view relative?\")\n",
    "print(is_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P_raw.copy()\n",
    "\n",
    "for view_name, row in P_raw.iterrows():\n",
    "    if is_relative[view_name]:\n",
    "        plus_tickers  = row[row >  0].index\n",
    "        minus_tickers = row[row <  0].index\n",
    "\n",
    "        # market weights\n",
    "        w_plus  = df_prior.loc[plus_tickers,  \"PriorWeight\"]\n",
    "        w_minus = df_prior.loc[minus_tickers, \"PriorWeight\"]\n",
    "\n",
    "        w_plus  = w_plus  / w_plus.sum()\n",
    "        w_minus = w_minus / w_minus.sum()\n",
    "\n",
    "        P.loc[view_name, plus_tickers]  =  w_plus.values\n",
    "        P.loc[view_name, minus_tickers] = -w_minus.values\n",
    "\n",
    "print(\"Final P matrix:\")\n",
    "print(P)\n",
    "P = P.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae869036",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([0.070, 0.020, 0.050, 0.012, 0.035]) # views vector\n",
    "C = np.array([0.60,  0.45,  0.55,  0.50,  0.65])  # confidence level vector\n",
    "\n",
    "# adjusting frequency\n",
    "Q = Q / WEEKS_PER_YEAR\n",
    "\n",
    "N, K = len(pi), len(Q) # assets, views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd9c71",
   "metadata": {},
   "source": [
    "## Part III - Uncertainty on Views\n",
    "<div style=\"float:left; margin:20px 0; width:500px;\">\n",
    "  <table style=\"border-collapse:collapse; font-family:Arial, sans-serif; font-size:14px;\">\n",
    "    <colgroup>\n",
    "      <col style=\"width:100px; text-align:center;\">\n",
    "      <col style=\"width:90px; text-align:center;\">\n",
    "      <col style=\"width:280px; text-align:left;\">\n",
    "    </colgroup>\n",
    "    <thead>\n",
    "      <tr style=\"background-color:#f2f2f2;\">\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Variable</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Shape</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Description</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\boldsymbol{\\mu}_{k,100\\%}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">BL Return (100% certainty) Vector</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{w}_{k,100\\%}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Optimal Weights (100% confidence)</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{D}_{k,100\\%}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Max Departures from Market Weights</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{Tilt}_k$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Tilt caused by $k$-th view</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{w}_{k,\\%}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Target weight vector</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\omega_k$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$1 \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Diagonal elements of $\\boldsymbol{\\Omega}$</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\boldsymbol{\\Omega}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$K \\times K$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Views Uncertainty Matrix</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb9ee2",
   "metadata": {},
   "source": [
    "Idzorek’s procedure quantifies view uncertainty by linking the tilt—the deviation from the equilibrium (market) portfolio implied by a fully certain view—to the investor’s stated confidence level. Formally, $$ \\mathbf{Tilt}_{k}\\;\\approx\\;\\bigl(\\mathbf{w}_{k,\\,100\\%}-\\mathbf{w}_{\\text{mkt}}\\bigr)\\,C_{k}$$ where $C_k$ is the stated confidence in view $k$, and $\\mathbf{w}_{k,100\\%}$ is the portfolio one would hold if that view were known with 100% certainty. Thus, a view’s weight can be interpreted as the deviation from the certainty‑view portfolio, scaled by the confidence level.\n",
    "\n",
    "Idzorek then provides a step‑by‑step method to compute the diagonal elements $\\omega_k$ of the view‑uncertainty matrix $\\boldsymbol{\\Omega}$, turning an otherwise abstract concept into quantities derived from tilts and confidences. Although the detailed procedures are omitted from this notebook, each step is highlighted in the code below. The complete list of procedures can be found in Appendix B of the accompanying PDF report.  \n",
    "The only techincal detailed worth mentioning here is the for step 6: this step consists in a minimzation problem under the constraint $\\omega_k > 0$. The definition of the problem is the following: $$\\min\\sum(\\mathbf{w}_{k,\\%} - \\mathbf{w}_k)^2$$ where $\\mathbf{w}_{k,\\%}$ is the target weight vector based on the tilt and $$\\mathbf{w}_k = \\left[2\\lambda_{\\text{mkt}}\\boldsymbol{\\Sigma}\\right]^{-1}\\left[\\boldsymbol{\\Sigma}^{-1}+\\frac{1}{\\omega_k}\\mathbf{p}_k' \\mathbf{p}_k\\right]^{-1}\\left[\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\pi} + \\frac{Q_k}{\\omega_k} \\mathbf{p}_k'\\right]$$ \n",
    "Here $\\mathbf{p}_k$ is $k$-th row of the pick matrix. $\\mathbf{p}_k' \\mathbf{p}_k$ is a column-by-row product, which produces a $(N\\times N)$ matrix, whose elements are $P_{i,j} = p_{k,i} \\times p_{k,j}$. The constrained minimization is performed using `minimize_scalar`, which is the simplest, fastest, and most robust choice for a one-dimensional problem. This iterative process leads to the matrix $\\tilde{\\Omega}_{kk} = \\omega_k$.  \n",
    "The final step is to scale the calibrated matrix $$\\boldsymbol{\\Omega} = \\tau \\, \\tilde{\\boldsymbol{\\Omega}}$$\n",
    "In this framework, $\\tau$ appears only as a constant factor that cancels algebraically in the posterior mean. Consequently, the combined expected return $\\boldsymbol{\\mu}_{\\text{BL}}$ is invariant to the specific value of $\\tau$, making its choice operationally irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e797f6",
   "metadata": {},
   "source": [
    "Note: Throughout the implementation, linear systems are solved using `np.linalg.solve` rather than by explicitly computing matrix inverses. The `solve` routine internally applies an LU decomposition of the coefficient matrix, which is both numerically more stable and computationally more efficient than forming $\\boldsymbol{\\Sigma}^{-1}$ explicitly. The only exception arises when the full inverse of $\\boldsymbol{\\Sigma}$ is required as a matrix (e.g., in the computation of implied weights), in which case `np.linalg.inv` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1  # results are independent from tau \n",
    "S_inv = np.linalg.inv(Sigma_lw)\n",
    "\n",
    "def implied_weights(p_k, Q_k, omega, lam):\n",
    "    lhs = S_inv + (1.0/omega) * np.outer(p_k, p_k)\n",
    "    rhs = S_inv @ pi + (Q_k/omega) * p_k\n",
    "    x   = np.linalg.solve(lhs, rhs)\n",
    "\n",
    "    return np.linalg.solve(2 * lam * Sigma_lw, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a959045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_matrix():\n",
    "    \"\"\"Idzorek procedure for Ω definition\"\"\"\n",
    "    mu_100    = np.zeros((K, N))\n",
    "    w_100     = np.zeros((K, N))\n",
    "    D_100     = np.zeros((K, N))\n",
    "    tilt      = np.zeros((K, N))\n",
    "    w_tilt    = np.zeros((K, N))\n",
    "    Omega     = np.zeros((K, K))\n",
    "    omega_opt = np.empty(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        p_k = P[k]  \n",
    "        Q_k = Q[k]  \n",
    "\n",
    "        num = Sigma_lw @ p_k           \n",
    "        den = p_k @ Sigma_lw @ p_k     \n",
    "        delta = Q_k - (p_k @ pi)\n",
    "\n",
    "        mu_100[k] = pi + (num / den) * delta # Step 1\n",
    "        w_100[k] = np.linalg.solve(2 * lambda_mkt * Sigma_lw, mu_100[k]) # Step 2\n",
    "        D_100[k] = w_100[k] - w_mkt # Step 3\n",
    "        tilt[k] = D_100[k] * C[k]   # Step 4\n",
    "        w_tilt[k] = w_mkt + tilt[k] # Step 5\n",
    "\n",
    "        def _objective(omega):\n",
    "            w_k = implied_weights(p_k, Q_k, omega, lambda_mkt)\n",
    "            return np.sum((w_k - w_tilt[k])**2)\n",
    "\n",
    "        res = minimize_scalar(_objective, bounds=(1e-8, 1e8), method='bounded')   \n",
    "        omega_opt[k] = res.x  # Step 6\n",
    "\n",
    "        Omega[k, k] = omega_opt[k]   \n",
    "    \n",
    "    return Omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0911196",
   "metadata": {},
   "source": [
    "## Part IV - BL Posterior and Weights \n",
    "<div style=\"float:left; margin:20px 0; width:500px;\">\n",
    "  <table style=\"border-collapse:collapse; font-family:Arial, sans-serif; font-size:14px;\">\n",
    "    <colgroup>\n",
    "      <col style=\"width:100px; text-align:center;\">\n",
    "      <col style=\"width:90px; text-align:center;\">\n",
    "      <col style=\"width:280px; text-align:left;\">\n",
    "    </colgroup>\n",
    "    <thead>\n",
    "      <tr style=\"background-color:#f2f2f2;\">\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Variable</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:center;\">Shape</th>\n",
    "        <th style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Description</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\boldsymbol{\\mu}_{\\text{BL}}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">BL Return Vector</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\lambda$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$1 \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">Investor's Risk Aversion</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$\\mathbf{w}_{\\text{BL}}$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:center;\">$N \\times 1$</td>\n",
    "        <td style=\"border:1px solid #ccc; padding:6px; text-align:left;\">BL Optimal Weights</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92669727",
   "metadata": {},
   "source": [
    "### Posterior returns \n",
    "For the posterior return vector, the stable representation proposed by Meucci (2010) is adopted: $$\\boldsymbol{\\mu}_{\\text{BL}} = \\boldsymbol{\\pi} + \\tau \\boldsymbol{\\Sigma} \\mathbf{P}' \\bigl(\\tau \\mathbf{P} \\boldsymbol{\\Sigma} \\mathbf{P}'+\\boldsymbol{\\Omega}\\bigr)^{-1} \\bigl(\\mathbf{Q}-\\mathbf{P}\\boldsymbol{\\pi}\\bigr)$$ Since $\\boldsymbol{\\Omega}$ was defined to be proportional to $\\tau$, it can be factored out and canceled from the posterior return.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_ret(Omega):\n",
    "    \"\"\"\n",
    "    Black-Litterman posterior return using stable representation.\n",
    "    \"\"\"\n",
    "    S_tau = tau * Sigma_lw \n",
    "\n",
    "    Kmat = P @ S_tau @ P.T + Omega * tau\n",
    "    rhs  = Q - P @ pi\n",
    "    adj  = np.linalg.solve(Kmat, rhs)          \n",
    "    mu_BL = pi + S_tau @ P.T @ adj             \n",
    "\n",
    "    return mu_BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega = uncertainty_matrix()  \n",
    "mu_BL = posterior_ret(Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04a92f",
   "metadata": {},
   "source": [
    "To illustrate the effect of the Black–Litterman update, the equilibrium returns $\\boldsymbol{\\pi}$, the posterior returns $\\boldsymbol{\\mu}_{\\text{BL}}$, and their differences $\\boldsymbol{\\mu}_{\\text{BL}} - \\boldsymbol{\\pi}$ are reported side‐by‐side (annualized). In addition, for each investor profile, the tilt vector $$ \\mathbf{Tilt} = \\mathbf{w}_{\\text{BL}} - \\mathbf{w}_{\\text{mkt}} $$ is shown. These tilts highlight how each portfolio deviates from the market‐capitalization benchmark, providing a direct view of how the incorporation of views shifts allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46415b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.DataFrame({\n",
    "    \"mu_BL [%]\"      : (mu_BL * WEEKS_PER_YEAR)* 100,                 \n",
    "    \"pi [%]\"         : (pi * WEEKS_PER_YEAR) *100,\n",
    "    \"mu_BL - pi [%]\" : ((mu_BL - pi) * WEEKS_PER_YEAR) * 100\n",
    "}, index=tickers)\n",
    "\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c1ea5",
   "metadata": {},
   "source": [
    "$\\lambda$ is the investor's risk-aversion coefficient. Three different investor categories are considered, Kelly ($\\lambda = 0.005$), Market ($\\lambda=1.12$) and Trustee ($\\lambda=3$). The values adopted are those suggested in the final project workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "investor_profile = [\"Kelly\", \"Average\", \"Trustee\"]\n",
    "lambda_df = pd.DataFrame(\n",
    "    {\"lambda\": [0.005, 1.12, 3.0]},\n",
    "    index=investor_profile\n",
    ")\n",
    "    \n",
    "investors = lambda_df.index.to_list()     \n",
    "lam_arr   = lambda_df[\"lambda\"].to_numpy()\n",
    "I = len(investors) # number of investor types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a3572",
   "metadata": {},
   "source": [
    "The optimal weights are then computed using the unconstrained mean-variance approach, which has the following analytical solution:\n",
    "$$\\mathbf{w}^* = \\frac{1}{2\\lambda}\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{\\text{BL}}$$\n",
    "This example illustrates the simplest form of portfolio optimization and is presented as a concise showcase. Later, in a dedicated section, this approach will be revisited by applying constraints, and comparing the results with other two techniques. The three weight sets obtained for the different investor profiles are stored together with equal–weight (`w_eq`) and market–weight (`w_mkt`) portfolios for comparison. Both raw and normalized weights are retained. Since equal and market weights are already positive and sum to one, the normalized form facilitates direct comparison, even though short positions may still occur in the unconstrained solutions before normalization. The Black-Litterman weights are distinguished by a suffix identifying the investor profile: `k` for Kelly, `a` for Average and `t` for Trustee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ee4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wBL_all = np.empty((I, N))\n",
    "for i, lam in enumerate(lam_arr):\n",
    "    wBL_all[i] = np.linalg.solve(2 * lam * Sigma_lw, mu_BL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f97b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame(index=tickers)\n",
    "weights_df['w_eq']  = np.full(N, 1/N)       \n",
    "weights_df['w_mkt'] = w_mkt                 \n",
    "\n",
    "suffix_list = {'Kelly': 'k', 'Average': 'a', 'Trustee': 't'}\n",
    "\n",
    "for row_idx, profile in enumerate(lambda_df.index):\n",
    "    suffix = suffix_list[profile]\n",
    "    weights_df[f'w_BL_{suffix}'] = wBL_all[row_idx]\n",
    "    weights_df[f'w_BL_{suffix}_norm'] = wBL_all[row_idx] / wBL_all[row_idx].sum()\n",
    "    weights_df[f'w_BL_{suffix}_tilt'] = weights_df[f'w_BL_{suffix}'] - weights_df['w_mkt']\n",
    "\n",
    "print(weights_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kelly: sum of weights {weights_df['w_BL_k'].sum().round(4)*100}%\")\n",
    "print(f\"Average: sum of weights {weights_df['w_BL_a'].sum().round(4)*100}%\")\n",
    "print(f\"Trustee: sum of weights {weights_df['w_BL_t'].sum().round(4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9693e2",
   "metadata": {},
   "source": [
    "Plotting all the normalzied weights collected for visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = ['w_eq','w_mkt','w_BL_k_norm','w_BL_a_norm','w_BL_t_norm']\n",
    "weights_norm = weights_df[cols_to_plot]\n",
    "\n",
    "labels  = ['Equal Weight', 'Market Cap',\n",
    "           'BL (Kelly)', 'BL (Average)', 'BL (Trustee)']\n",
    "colors  = ['lightgray', 'orange', '#6baed6', '#31a354', '#756bb1']\n",
    "\n",
    "x = np.arange(N)\n",
    "bar_width = 0.8 / len(cols_to_plot)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (col, lab, colr) in enumerate(zip(cols_to_plot, labels, colors)):\n",
    "    ax.bar(\n",
    "        x + (i - (len(cols_to_plot)-1)/2) * bar_width,\n",
    "        weights_norm[col].values,\n",
    "        width  = bar_width,\n",
    "        label  = lab,\n",
    "        color  = colr,\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(weights_norm.index)\n",
    "for xi in x[:-1]:\n",
    "    ax.axvline(x=xi + 0.5, linestyle='--', linewidth=0.5,\n",
    "               color='gray', alpha=0.7)\n",
    "\n",
    "ax.set_xlim(-0.5, N - 0.5)\n",
    "ax.set_ylabel('Normalized weight')\n",
    "ax.set_title('Portfolio Allocations (Normalized)')\n",
    "ax.legend(ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b35059",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273df64d",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df105330",
   "metadata": {},
   "source": [
    "Three optimization methods are examined:  \n",
    "1. **Mean–Variance (Markowitz)** \n",
    "2. **Maximum Sharpe Ratio**  \n",
    "3. **Risk Budgeting**\n",
    "\n",
    "For consistency and to address robustness concerns, each method is subject to the following constraints:  \n",
    "- **Budget constraint**:  $$\\sum_{i=1}^{N} \\text{w}_i = 1$$\n",
    "- **Box constraint** (no shorting or leverage): $$0 \\le \\text{w}_i \\le 1 \\quad \\forall\\,i = 1,2,\\dots,N$$\n",
    "\n",
    "The $\\text{w}_i \\le 1$ condition is mathematically redundant if the budget and non-negativity constraints are satisfied, but it is still reported as a diagnostic to catch potential numerical violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293eec5",
   "metadata": {},
   "source": [
    "Analysis is restricted to the \"Average\" investor profile, with the risk-aversion parameter fixed at $\\lambda = 1.12$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_avg = lambda_df.loc[\"Average\", \"lambda\"]\n",
    "\n",
    "opt_weights = pd.DataFrame()\n",
    "opt_weights['w_eq'] = weights_df['w_eq']\n",
    "opt_weights['w_mkt'] = weights_df['w_mkt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf60a3",
   "metadata": {},
   "source": [
    "### Performance assessment\n",
    "\n",
    "Algorithm performance is evaluated on three features: **feasibility**, **numerical quality**, and **robustness**.\n",
    "\n",
    "- **Feasibility** — Solutions are checked against the budget and box constraints with tight tolerances. The following violations are reported, which should be numerically close to zero for a valid optimum: $$\\underbrace{|1 - \\mathbf{1}' \\mathbf{w}|}_{\\text{budget}},\\;\\;\\;\\;\\underbrace{\\max(0,-\\min_i \\text{w}_i)}_{\\text{no shorts}}, \\;\\;\\;\\; \\underbrace{\\max(0,\\max_i \\text{w}_i-1)}_{\\text{no over-weights}}$$ In practice, violations $\\le 10^{-8}$ are considered \"tight\". \n",
    "\n",
    "- **Numerical quality** - Method-specific metrics are reported to quantify how well the computed solution optimizes its stated objective: \n",
    "    - *mean-variance*: primal objective, dual objective, and primal–dual gap.\n",
    "    - *max sharpe*: Dinkelbach residual, final inner solver first-order optimality and maximum constraint violation.\n",
    "    - *risk budgeting*: final value of the risk-budgeting objective.\n",
    "\n",
    "- **Robustness** — Stability is assessed by re-solving after small, controlled perturbations of $(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$. For each method, $R$ independent perturbations are generated and solved; here $R=5$, meaning that each optimisation is re-run on five distinct jittered inputs. The distribution of resulting weights, objective values, and solve times is summarised. Robust algorithms exhibit small dispersion: solutions and times should not vary erratically under mild input noise. Perturbations are generated as follows: \n",
    "    1. Symmetrization and Positive Semi-Definiteness (PSD) enforcement - The covariance estimate is first symmetrized, then shifted on the diagonal if its smallest eigenvalue is below a tolerance $\\varepsilon$. Concretely, if $\\lambda_{\\min}(\\boldsymbol{\\Sigma})<\\varepsilon$ then set $\\boldsymbol{\\Sigma} \\leftarrow \\boldsymbol{\\Sigma} + (\\varepsilon-\\lambda_{\\min})\\mathbb{I}$, which ensures $\\lambda_{\\min}(\\boldsymbol{\\Sigma})\\ge \\varepsilon$ (hence $\\boldsymbol{\\Sigma}$ is PSD).\n",
    "    2. Mean jitter - perturb expected returns with i.i.d. Gaussian noise scaled by the cross‑sectional dispersion of $\\boldsymbol{\\mu}$: $$\\boldsymbol{\\mu}_j = \\boldsymbol{\\mu} + \\mu_{scale} \\cdot \\max(10^{-12}, \\text{std}_{\\mu} )\\epsilon_{\\mu},\\;\\;\\;\\;\\epsilon_{\\mu}\\sim\\mathcal{N}(0,1)$$ By default, $\\mu_{\\text{scale}}=1\\%$.\n",
    "    3. Covariance jitter: draw a symmetric noise matrix $\\mathbf{E}=(\\mathbf{Z}+\\mathbf{Z}')/2$ with $Z_{ij}\\sim\\mathcal{N}(0,1)$ and scale it by the average covariance level using the Frobenius norm: $$\\text{scale} = \\sigma_{scale}\\frac{||\\boldsymbol{\\Sigma}||_2}{N},\\;\\;\\;\\; \\text{Proj}_{\\text{PSD}}=(\\boldsymbol{\\Sigma} + \\text{scale} \\cdot \\mathbf{E})$$ where by default $\\sigma_{\\text{scale}}=1%$ and $\\operatorname{Proj}_{\\text{PSD}}$ denotes the eigenvalue‑flooring step in (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertubation functions\n",
    "def _ensure_psd(S, eps=1e-10):\n",
    "    S = (S + S.T)/2\n",
    "    w, V = np.linalg.eigh(S)\n",
    "    m = w.min()\n",
    "    if m < eps:\n",
    "        S = S + (eps - m) * np.eye(S.shape[0])\n",
    "    return (S + S.T)/2\n",
    "\n",
    "def _jitter_inputs(mu, Sigma, mu_scale=0.01, sig_scale=0.01, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    # jitter mu \n",
    "    mu_j = mu + mu_scale * max(1e-12, float(np.std(mu))) * rng.standard_normal(mu.shape)\n",
    "    \n",
    "    # jitter Sigma \n",
    "    E = rng.standard_normal(Sigma.shape); E = (E + E.T)/2\n",
    "    scale = sig_scale * (np.linalg.norm(Sigma, 'fro') / Sigma.shape[0]) \n",
    "    Sigma_j = _ensure_psd(Sigma + scale * E)\n",
    "    \n",
    "    return mu_j, Sigma_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22940b0",
   "metadata": {},
   "source": [
    "For each of the $R$ jittered runs, the robustness routine records:\n",
    "- `iters` — solver iteration count.\n",
    "- `gap` — primal–dual optimality gap at termination.\n",
    "- `feas` — budget constraint violation $|1 - \\mathbf{1}' \\mathbf{w}|$\n",
    "- `lb` — lower-bound violation $\\max(0,-\\min_i \\text{w}_i)$\n",
    "- `ub` — upper-bound violation $\\max(0,\\max_i \\text{w}_i-1)$ (redundant but included for diagnostics).\n",
    "- `wL1` — $\\ell^1$ distance from baseline weights: total absolute reallocation caused by jitter.\n",
    "- `wLinf` — $\\ell^{\\infty}$ distance from baseline weights: largest single‐asset change.\n",
    "\n",
    "Summary statistics are then computed: medians for runtimes, iteration counts, gaps, and weight deviations; maxima for feasibility violations, to report the worst‐case drift observed across the $R$ perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf85304",
   "metadata": {},
   "source": [
    "## Markowitz Mean–Variance\n",
    "\n",
    "With the constraints defined, the portfolio choice problem becomes a Quadratic Program (QP): a quadratic objective function subject to linear constraints.  \n",
    "The utility function is  \n",
    "$$\n",
    "U(\\mathbf{w}) = \\mathbf{w}'\\boldsymbol{\\mu}_{\\text{BL}} - \\lambda \\mathbf{w}' \\boldsymbol{\\Sigma}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "The `CVXOPT` package is used to solve this via its `cvxopt.solvers.qp` function, which addresses problems of the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underset{\\mathbf{x}}{\\mathrm{minimize}}\n",
    "&& \\tfrac12\\,\\mathbf{x}'\\mathbf{H}\\,\\mathbf{x} \\;+\\; \\mathbf{f}' \\\\[6pt]\n",
    "&\\text{subject to}\n",
    "&& \\mathbf{G}\\,\\mathbf{x} \\;\\le\\; \\mathbf{h},\\\\\n",
    "&&& \\mathbf{A}\\,\\mathbf{x} = b.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All inputs are provided as `cvxopt.matrix` objects. Note that even though they are named \"matrix\" objects they can also be vectors or scalars. To match the Markowitz objective, $\\mathbf{H} = 2\\,\\lambda\\,\\boldsymbol{\\Sigma}$ while $\\mathbf{f} = -\\boldsymbol{\\mu}_{\\text{BL}}$.  \n",
    "The budget constraint implementation is straghtforward ($\\mathbf{A}=[1,1,\\dots,1],\\;b=1$ ), while the box constraints $0 \\le x_i \\le 1$ require a little more structure:\n",
    "\n",
    "- $\\mathbf{G}$ is a $(2N \\times N)$ matrix formed by stacking the $(N\\times N)$ identity matrix above its negation:\n",
    "  $$\n",
    "    \\mathbf{G} = \\begin{pmatrix}\n",
    "      \\mathbb{I}_N \\\\[4pt]\n",
    "      -\\mathbb{I}_N\n",
    "    \\end{pmatrix}\n",
    "  $$\n",
    "- $\\mathbf{h}$ is a $2N$-vector with the first $N$ entries equal to 1 and the last $N$ entries equal to 0:\n",
    "  $$\n",
    "    \\mathbf{h} = \\begin{pmatrix}\n",
    "      \\mathbf{1}_N \\\\[4pt]\n",
    "      \\mathbf{0}_N\n",
    "    \\end{pmatrix}\n",
    "  $$\n",
    "\n",
    "Matrix multiplication then leads to  \n",
    "$$\n",
    "\\mathbb{I}_N\\,x \\;\\le\\; \\mathbf{1}_N \\quad\\Longrightarrow\\quad x_i \\le 1,\\;\\;\\forall \\,i \\\\\n",
    "-\\,\\mathbb{I}_N\\,x \\;\\le\\; \\mathbf{0}_N \\quad\\Longrightarrow\\quad -x_i \\le 0,\\;\\;\\forall \\,i,\n",
    "$$  \n",
    "which together impose $0 \\le x_i \\le 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c7a39",
   "metadata": {},
   "source": [
    "Numerical quality is assessed from the solver’s status flag, the number of iterations to convergence, and the duality gap. The primal objective corresponds to the QP itself, while the dual objective is obtained from the Lagrangian dual problem. `CVXOPT` reports both values: for any feasible problem, $$\\text{primal obj} \\geq \\text{dual obj}$$ and at optimality the duality gap $$\\text{gap} = \\text{primal obj} - \\text{dual obj} \\approx 0$$ A small gap certifies that the computed solution is close to optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_variance_qp(mu, Sigma, lam):\n",
    "    \"\"\"\n",
    "    Solve a long-only, fully-invested mean-variance optimization via CVXOPT QP.\n",
    "\n",
    "    Inputs:\n",
    "        mu    : excess return vector\n",
    "        Sigma : covariance matrix of excess returns\n",
    "        lam   : risk aversion parameter\n",
    "\n",
    "    Output:\n",
    "        dict with solver status, and optimal weights\n",
    "    \"\"\"\n",
    "\n",
    "    H = 2 * lam * Sigma\n",
    "    f = -mu\n",
    "\n",
    "    H_cvx = matrix(H)\n",
    "    f_cvx = matrix(f)\n",
    "\n",
    "    # Bounds \n",
    "    G_cvx = matrix(np.vstack((np.eye(N), -np.eye(N))))\n",
    "    h_cvx = matrix(np.hstack((np.ones(N), np.zeros(N))))\n",
    "\n",
    "    # Budget constraint\n",
    "    A_cvx = matrix(np.ones((1, N)))\n",
    "    b_cvx = matrix(1.0)\n",
    "\n",
    "    solvers.options.update({\n",
    "        'show_progress': False,\n",
    "        'abstol': 1e-9,\n",
    "        'reltol': 1e-9,\n",
    "        'feastol': 1e-9,\n",
    "        'maxiters': 100\n",
    "    })\n",
    "\n",
    "    # Solving QP\n",
    "    sol = solvers.qp(H_cvx, f_cvx, G_cvx, h_cvx, A_cvx, b_cvx)\n",
    "\n",
    "    # Numerical quality\n",
    "    w = np.array(sol['x']).ravel()\n",
    "    status = sol['status']\n",
    "    iters = sol['iterations']\n",
    "    primal = sol['primal objective']\n",
    "    dual   = sol['dual objective']\n",
    "    gap    = primal - dual  \n",
    "\n",
    "    # Feasibility checks\n",
    "    feas_budget = abs(w.sum() - 1.0)\n",
    "    feas_lower  = max(0.0, -w.min())\n",
    "    feas_upper  = max(0.0,  w.max() - 1.0)\n",
    "\n",
    "    # Outputs\n",
    "    report = {\n",
    "        'status': status,\n",
    "        'iters': iters,\n",
    "        'primal_obj': primal,\n",
    "        'dual_obj': dual,\n",
    "        'gap': gap,\n",
    "        'feas_budget': feas_budget,\n",
    "        'feas_lb_viol': feas_lower,\n",
    "        'feas_ub_viol': feas_upper,\n",
    "        'weights': w\n",
    "    }\n",
    "    return report\n",
    "\n",
    "rep = mean_variance_qp(mu_BL, Sigma_lw, lambda_avg)\n",
    "\n",
    "print(f\"Status: {rep['status']}, iters: {rep['iters']}\")\n",
    "print(f\"Feasibility  | budget: {rep['feas_budget']:.2e}, \"\n",
    "      f\"lb: {rep['feas_lb_viol']:.2e}, ub: {rep['feas_ub_viol']:.2e}\")\n",
    "print(f\"Objectives   | primal: {rep['primal_obj']:.8f}, dual: {rep['dual_obj']:.8f}, gap: {rep['gap']:.2e}\")\n",
    "\n",
    "opt_weights['w_mv'] = rep['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf91e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness \n",
    "def mv_input_sensitivity(mu, Sigma, lam, R=5, mu_scale=0.01, sig_scale=0.01, seed=0):\n",
    "    \"\"\"\n",
    "    Assess mean-variance solution sensitivity to small perturbations in μ and Σ.\n",
    "\n",
    "    Inputs:\n",
    "        mu, Sigma : base excess returns and covariance\n",
    "        lam       : risk aversion parameter\n",
    "        R         : number of jittered runs\n",
    "\n",
    "    Output:\n",
    "        tuple of (all run results DataFrame, aggregated summary, base run report)\n",
    "    \"\"\"\n",
    "    \n",
    "    base = mean_variance_qp(mu, Sigma, lam)\n",
    "    w0 = base['weights']\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # jitters\n",
    "    for r in range(R):\n",
    "        mu_j, Sig_j = _jitter_inputs(mu, Sigma, mu_scale, sig_scale, rng)\n",
    "        rep = mean_variance_qp(mu_j, Sig_j, lam)\n",
    "        rows.append(dict(\n",
    "            run=r,\n",
    "            iters=rep['iters'],\n",
    "            gap=rep['gap'],\n",
    "            feas=rep['feas_budget'],\n",
    "            lb=rep['feas_lb_viol'],\n",
    "            ub=rep['feas_ub_viol'],\n",
    "            wL1=float(np.sum(np.abs(rep['weights'] - w0))),\n",
    "            wLinf=float(np.max(np.abs(rep['weights'] - w0)))\n",
    "        ))\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = df.agg({\n",
    "        'iters':'median', 'gap':'median',\n",
    "        'feas':'max', 'lb':'max', 'ub':'max',\n",
    "        'wL1':'median', 'wLinf':'median'\n",
    "    }).rename({'feas':'feas_max', 'lb':'lb_max', 'ub':'ub_max'})\n",
    "    print(\"Mean-Variance: input sensitivity (medians / max violations)\")\n",
    "    print(summary.round(6).to_string())\n",
    "    return df, summary, base\n",
    "\n",
    "df_rob, sum_rob, mv_base = mv_input_sensitivity(mu_BL, Sigma_lw, lambda_avg, R=5, mu_scale=0.01, sig_scale=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce340a",
   "metadata": {},
   "source": [
    "## Maximum Sharpe Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2df5a5",
   "metadata": {},
   "source": [
    "This optimization strategy aims to maximize the portfolio's Sharpe ratio. The utility function is $$\\text{SR}(\\mathbf{w}) = \\frac{\\mathbf{w}'\\boldsymbol{\\mu}_{\\text{BL}}}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}$$\n",
    "Due to the square root at the denominator this problem is not convex, so it isn't possible to use the same QP technique as before. However, it belongs to a particular class called *fractional programs*, which can be addressed numerically with iterative methods. Here, Dinkelbach’s algorithm is applied. This method transforms the fractional objective into a sequence of convex subproblems:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underset{\\mathbf{w}}{\\mathrm{maximize}}\n",
    "&& \\,\\mathbf{w}'\\boldsymbol{\\mu}_{\\text{BL}} - y_{(k)} \\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma} \\mathbf{w}}\\\\[6pt]\n",
    "&\\text{subject to}\n",
    "&& \\sum_i\\text{w}_i = 1 \\\\\n",
    "&&& \\text{w}_i > 0\\;\\;\\forall \\,i = 1,2,\\dots,N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the parameter $$y_{(k)} = \\frac{\\mathbf{w}_{(k)}'\\boldsymbol{\\mu}_{\\text{BL}}}{\\sqrt{\\mathbf{w}_{(k)}' \\boldsymbol{\\Sigma}\\mathbf{w}_{(k)}}}$$ will be updated at each iteration. In this equation the subscript $(k)$ is the iteration index, not the element of a vector.\n",
    "\n",
    "The algorithm proceeds as follows:\n",
    "- **Step 0**: Initialize $\\mathbf{w}_{(0)}$ (e.g., equal weights) and compute $y_{(0)}$.\n",
    "- **Step 1**: Solve the convex problem defined above to find the new set of weights. These will be passed to the next iteration to improve convergence speed (warm start).\n",
    "The convex problem is solved by changing the sign of the objective function and using `scipy.minimize` with the method `trust-constr`, which offers a good balance of robustness and speed. This method requires the budget constraint to be passed as a `LinearConstraint` object with three arguments: the row vector $\\mathbf{A}$ of shape $(1 \\times N)^{\\dagger}$, the lower bound (`lb`) and the upper bound (`ub`), which is then interpeted as $\\text{lb} \\le \\mathbf{A}\\,\\mathbf{x}\\le \\text{ub}$ (where $\\mathbf{x}$ is the independent variables vector). Equality constraints can be obtained using equal bounds. The solver also accepts the objective Jacobian, which materially reduces runtime.\n",
    "- **Step 2**: Update $y_{(k+1)}$ \n",
    "- **Step 3**: Convergence check: if $|y_{(k+1)} - y_{(k)}|<\\varepsilon$ terminate; otherwise repeat from step 1 until convergence.\n",
    "\n",
    "In practice, the procedure converges in a small number of outer iterations.\n",
    "\n",
    "$^\\dagger$ The class actually accepts a matrix of shape $(m \\times n)$, where $m$ is the number of constraints and $n$ the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad38d7",
   "metadata": {},
   "source": [
    "Numerical quality is evaluated from these diagnostics: \n",
    "- Dinkelbach residual: $r = |y_{(k+1)}-y_{(k)}|$\n",
    "- Final inner solver optimality — first-order optimality measure (`optimality`) from the last `trust-constr` solve.\n",
    "- Final inner solver constraint violation — maximum absolute constraint violation (`constr_violation`) from the last inner solve.\n",
    "\n",
    "Performance metrics are reported separately: outer iteration count and last/total inner iteration counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner problem \n",
    "def solve_convex(mu, Sigma, y, w0, bounds, cons):\n",
    "    \"\"\"\n",
    "    Solve the convex subproblem in the Dinkelbach Max Sharpe method.\n",
    "    \n",
    "    Inputs:\n",
    "        mu, Sigma : excess returns and covariance\n",
    "        y         : current Sharpe ratio estimate\n",
    "        w0        : starting weights\n",
    "        bounds    : box constraints\n",
    "        cons      : budget constraints\n",
    "\n",
    "    Output:\n",
    "        SciPy OptimizeResult of the inner solve\n",
    "    \"\"\"\n",
    "\n",
    "    def _obj(w):\n",
    "        return -(w @ mu - y * np.sqrt(w @ Sigma @ w))\n",
    "    def _jac(w):\n",
    "        v = w @ Sigma @ w\n",
    "        s = np.sqrt(max(v, 1e-18)) # avoid zero division\n",
    "        return -(mu - y * (Sigma @ w) / s)\n",
    "    res = minimize(_obj, w0, \n",
    "                   method='trust-constr', \n",
    "                   jac=_jac,\n",
    "                   bounds=bounds, \n",
    "                   constraints=cons,\n",
    "                   options={'maxiter': 500})\n",
    "    if not res.success:\n",
    "        raise RuntimeError(\"Inner solve failed: \" + res.message)\n",
    "    return res\n",
    "\n",
    "def find_y(mu, Sigma, w):\n",
    "    return (w @ mu) / np.sqrt(max(w @ Sigma @ w, 1e-18)) # avoid zero division\n",
    "\n",
    "# Dinkelbach algorithm (outer problem)\n",
    "def max_sharpe_report(mu, Sigma, w0, bounds, cons, tol=1e-6, max_outer=30):\n",
    "    \"\"\"\n",
    "    Compute the maximum Sharpe ratio portfolio using the Dinkelbach algorithm.\n",
    "    \n",
    "    Inputs:\n",
    "        mu, Sigma : excess returns and covariance\n",
    "        w0        : initial weights\n",
    "        bounds    : box constraints\n",
    "        cons      : budget constraints\n",
    "\n",
    "    Output:\n",
    "        dict with iteration counts, feasibility checks, and optimal weights\n",
    "    \"\"\"\n",
    "    \n",
    "    y = find_y(mu, Sigma, w0)\n",
    "    w = w0.copy()\n",
    "    inner_nits = []\n",
    "    last_inner = None\n",
    "    dres = np.nan\n",
    "\n",
    "    for k in range(max_outer):\n",
    "        res = solve_convex(mu, Sigma, y, w, bounds, cons)\n",
    "        w_new = res.x\n",
    "        inner_nits.append(res.nit)\n",
    "        last_inner = res\n",
    "\n",
    "        y_new = find_y(mu, Sigma, w_new)\n",
    "        dres = abs(y_new - y)\n",
    "        w, y = w_new, y_new\n",
    "        if dres < tol:\n",
    "            break\n",
    "\n",
    "    # feasibility\n",
    "    feas_budget = abs(1.0 - w.sum())\n",
    "    feas_lower  = max(0.0, -w.min())\n",
    "    feas_upper  = max(0.0,  w.max() - 1.0)\n",
    "\n",
    "    # numerical quality (outer/inner + residuals)\n",
    "    outer_iters = k + 1\n",
    "    inner_total = int(np.sum(inner_nits))\n",
    "    inner_last  = int(inner_nits[-1]) if inner_nits else 0\n",
    "    inner_opt   = getattr(last_inner, 'optimality', np.nan)\n",
    "    inner_cviol = getattr(last_inner, 'constr_violation', np.nan)\n",
    "\n",
    "    # Outputs\n",
    "    report = {\n",
    "        'weights': w,\n",
    "        'outer_iters': outer_iters,\n",
    "        'inner_iters_last': inner_last,\n",
    "        'inner_iters_total': inner_total,\n",
    "        'dinkelbach_residual': dres,          \n",
    "        'feas_budget': feas_budget,\n",
    "        'feas_lb_viol': feas_lower,\n",
    "        'feas_ub_viol': feas_upper,\n",
    "        'inner_optimality': inner_opt,       \n",
    "        'inner_constr_violation': inner_cviol,\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "bounds = [(0.0, 1.0)] * N\n",
    "cons   = LinearConstraint(np.ones((1, N)), 1.0, 1.0)\n",
    "w0     = np.full(N, 1.0/N)\n",
    "\n",
    "rep = max_sharpe_report(mu_BL, Sigma_lw, w0, bounds, cons)\n",
    "\n",
    "print(f\"Outer iters: {rep['outer_iters']}, inner (last/total): {rep['inner_iters_last']} / {rep['inner_iters_total']}\")\n",
    "print(f\"Feasibility  | budget: {rep['feas_budget']:.2e}, lb viol: {rep['feas_lb_viol']:.2e}, ub viol: {rep['feas_ub_viol']:.2e}\")\n",
    "print(f\"Numerical    | Dinkelbach residual: {rep['dinkelbach_residual']:.2e}, \"\n",
    "      f\"inner optimality: {rep['inner_optimality']:.2e}, inner constr viol: {rep['inner_constr_violation']:.2e}\")\n",
    "\n",
    "opt_weights['w_ms'] = rep['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0097471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness\n",
    "def ms_input_sensitivity(mu, Sigma, w0, bounds, cons, R=5, mu_scale=0.01, sig_scale=0.01, seed=0):\n",
    "    \"\"\"\n",
    "    Test sensitivity of the Max Sharpe solution to small perturbations in μ and Σ.\n",
    "\n",
    "    Inputs:\n",
    "        mu, Sigma : base excess returns and covariance\n",
    "        w0        : initial weights\n",
    "        bounds    : box constraints\n",
    "        cons      : budget constraints\n",
    "        R         : number of jittered runs\n",
    "\n",
    "    Output:\n",
    "        tuple (runs DataFrame, aggregated summary, base run report)\n",
    "    \"\"\"\n",
    "    \n",
    "    base = max_sharpe_report(mu, Sigma, w0, bounds, cons)\n",
    "    w_base = base['weights']\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for r in range(R):\n",
    "        mu_j, Sig_j = _jitter_inputs(mu, Sigma, mu_scale, sig_scale, rng)\n",
    "        rep = max_sharpe_report(mu_j, Sig_j, w0, bounds, cons)\n",
    "        rows.append(dict(\n",
    "            run=r,\n",
    "            outer=rep['outer_iters'],\n",
    "            inner=rep['inner_iters_total'],\n",
    "            dres=rep['dinkelbach_residual'],\n",
    "            feas=rep['feas_budget'],\n",
    "            lb=rep['feas_lb_viol'],\n",
    "            ub=rep['feas_ub_viol'],\n",
    "            wL1=float(np.sum(np.abs(rep['weights'] - w_base))),\n",
    "            wLinf=float(np.max(np.abs(rep['weights'] - w_base)))\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = df.agg({\n",
    "        'outer':'median', 'inner':'median', 'dres':'median',\n",
    "        'feas':'max', 'lb':'max', 'ub':'max', 'wL1':'median', 'wLinf':'median'\n",
    "    }).rename({'feas':'feas_max','lb':'lb_max','ub':'ub_max'})\n",
    "\n",
    "    print(\"Max-Sharpe: μ, Σ input sensitivity (medians / max violations)\")\n",
    "    print(summary.round(6).to_string())\n",
    "    return df, summary, base\n",
    "\n",
    "df_ms, sum_ms, base_ms = ms_input_sensitivity(\n",
    "    mu_BL, Sigma_lw, w0, bounds, cons,\n",
    "    R=5, mu_scale=0.01, sig_scale=0.01, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ed1a6",
   "metadata": {},
   "source": [
    "## Risk budgeting\n",
    "\n",
    "In this approach, portfolio weights are chosen so that each asset’s contribution to portfolio Expected Shortfall (ES) matches a target “budget” vector $\\boldsymbol{\\beta}$.  \n",
    "First, compute the ES tail factor $$\\varphi_{\\text{ES}} = \\frac{\\phi (z_c)}{c}, \\;\\;\\;\\; z_c = \\Phi^{-1}(1-c)$$ \n",
    "for confidence level $c$, where $\\Phi$ is the standard normal CDF. In line with the Fundamental Review of the Trading Book (FRTB) directives, $c=0.025$ is used, corresponding to a 97.5% confidence level.\n",
    "\n",
    "Given weights $\\mathbf{w}$, the ES risk contribution of asset $i$ is\n",
    "$$\\text{RC}_i (\\mathbf{w}) = \\text{w}_i \\left( -\\mu_{\\text{BL},i} + \\varphi_{\\text{ES}} \\cdot \\frac{\\left(\\boldsymbol{\\Sigma} \\mathbf{w}\\right)_i}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\\right)$$\n",
    "and the corresponding share is\n",
    "$$S_i = \\frac{\\text{RC}_i}{\\sum_j \\text{RC}_j},$$\n",
    "so that $\\sum_i S_i = 1$.\n",
    "\n",
    "Weights are obtained by minimizing the squared deviation between the normalized contributions and the target budgets, under the usual constraints\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{\\text{w}} & \\sum_i \\bigl(S_i - \\beta_i\\bigr)^2\\\\[6pt]\n",
    "\\text{subject to} \\;\\;\\;& \\sum_i\\text{w}_i = 1 \\\\\n",
    " &\\text{w}_i > 0\\;\\;\\forall \\,i = 1,2,\\dots,N.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This yields a risk-budgeted allocation whose ES contribution shares match $\\boldsymbol{\\beta}$.\n",
    "The problem is nonlinear and non-convex, so a Sequential Quadratic Programming (SQP) scheme is adopted. In SciPy, `SLSQP` implements SQP by, at each iteration, building a quadratic model of the Lagrangian and a first-order linearization of the constraints; the resulting quadratic program provides a search direction that is accepted via a line-search/merit function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee36592",
   "metadata": {},
   "source": [
    "Numerical quality is evaluated based on the solver’s reported status (`success`), the achieved objective value (smaller is better, ideally near zero), and feasibility checks for the final weights (budget and box violations, as in the general section). Solver tolerances are those passed to SLSQP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9343e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES tail factor\n",
    "def es_factor(c=0.025):  # 97.5% tail\n",
    "    zc = norm.ppf(1 - c)\n",
    "    return norm.pdf(zc) / c\n",
    "\n",
    "# Risk contribution vector\n",
    "def rc_vector_es(w, mu, Sigma, factor):\n",
    "    port_sd     = np.sqrt(w @ Sigma @ w)\n",
    "    marginal_sd = (Sigma @ w) / port_sd\n",
    "    return w * (-mu + factor * marginal_sd)\n",
    "\n",
    "def es_risk_budgeting_contributions(mu, Sigma, c=0.025, budget=None, tol=1e-12, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Compute an Expected Shortfall (ES) risk-budgeting portfolio via SLSQP.\n",
    "\n",
    "    Inputs:\n",
    "        mu, Sigma : excess returns and covariance\n",
    "        c         : ES confidence level (e.g., 0.025 for 97.5% ES)\n",
    "        budget    : target risk budget vector (defaults to equal budget)\n",
    "\n",
    "    Output:\n",
    "        dict with iteration count, feasibility checks, and optimal weights\n",
    "    \"\"\"\n",
    "    \n",
    "    mu    = np.asarray(mu, dtype=float)\n",
    "    Sigma = np.asarray(Sigma, dtype=float)\n",
    "    N     = mu.size\n",
    "\n",
    "    # budget weights (market)\n",
    "    budget = np.asarray(budget, dtype=float)\n",
    "\n",
    "    factor = es_factor(c)\n",
    "\n",
    "    def _objective(w):\n",
    "        rc    = rc_vector_es(w, mu, Sigma, factor)\n",
    "        total = rc.sum()\n",
    "        return np.sum((rc/total - budget)**2)\n",
    "\n",
    "    # constraints\n",
    "    cons   = ({'type':'eq', 'fun': lambda w: np.sum(w) - 1.0},)  \n",
    "    bounds = [(0.0, 1.0)] * N                                    \n",
    "    w0     = np.full(N, 1.0/N)\n",
    "\n",
    "    res = minimize(_objective, w0, method=\"SLSQP\",\n",
    "                   bounds=bounds, constraints=cons,\n",
    "                   options={'ftol': tol, 'maxiter': maxiter})\n",
    "\n",
    "    if not res.success:\n",
    "        raise RuntimeError(\"ES risk-budgeting failed: \" + res.message)\n",
    "\n",
    "    w = res.x\n",
    "\n",
    "    # feasibility checks\n",
    "    feas_budget = abs(1.0 - w.sum())\n",
    "    feas_lower  = max(0.0, -w.min())\n",
    "    feas_upper  = max(0.0,  w.max() - 1.0)\n",
    "\n",
    "    # Outputs\n",
    "    report = {\n",
    "        'weights': w,\n",
    "        'nit': res.nit,\n",
    "        'feas_budget': feas_budget,\n",
    "        'feas_lb_viol': feas_lower,\n",
    "        'feas_ub_viol': feas_upper,\n",
    "        'objective': _objective(w)\n",
    "    }\n",
    "\n",
    "    return report\n",
    "\n",
    "rep = es_risk_budgeting_contributions(\n",
    "    mu=mu_BL, Sigma=Sigma_lw, c=0.025, budget=w_mkt\n",
    ")\n",
    "\n",
    "print(f\"SLSQP iterations: {rep['nit']}\")\n",
    "print(f\"Feasibility | budget: {rep['feas_budget']:.2e}, \"\n",
    "      f\"lb viol: {rep['feas_lb_viol']:.2e}, ub viol: {rep['feas_ub_viol']:.2e}\")\n",
    "\n",
    "opt_weights['w_rb'] = rep['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28497ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness\n",
    "def rb_input_sensitivity(mu, Sigma, budget, R=5, mu_scale=0.01, sig_scale=0.01, seed=0, c=0.025):\n",
    "    \"\"\"\n",
    "    Test sensitivity of the ES risk-budgeting solution to small perturbations in μ and Σ.\n",
    "\n",
    "    Inputs:\n",
    "        mu, Sigma : base excess returns and covariance\n",
    "        budget    : target risk budget vector\n",
    "        R         : number of jittered runs\n",
    "        c         : ES confidence level\n",
    "\n",
    "    Output:\n",
    "        tuple (runs DataFrame, aggregated summary, base run report)\n",
    "    \"\"\"\n",
    "    \n",
    "    base = es_risk_budgeting_contributions(mu, Sigma, c=c, budget=budget)\n",
    "    w_base = base['weights']\n",
    "\n",
    "    rows = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for r in range(R):\n",
    "        mu_j, Sig_j = _jitter_inputs(mu, Sigma, mu_scale, sig_scale, rng)  # uses your existing helper\n",
    "        rep = es_risk_budgeting_contributions(mu_j, Sig_j, c=c, budget=budget)\n",
    "        rows.append(dict(\n",
    "            run=r,\n",
    "            nit=rep['nit'],\n",
    "            feas=rep['feas_budget'],\n",
    "            lb=rep['feas_lb_viol'],\n",
    "            ub=rep['feas_ub_viol'],\n",
    "            obj=rep['objective'],\n",
    "            wL1=float(np.sum(np.abs(rep['weights'] - w_base))),\n",
    "            wLinf=float(np.max(np.abs(rep['weights'] - w_base)))\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    summary = df.agg({\n",
    "        'nit':'median', 'obj':'median',\n",
    "        'feas':'max', 'lb':'max', 'ub':'max',\n",
    "        'wL1':'median', 'wLinf':'median'\n",
    "    }).rename({'feas':'feas_max','lb':'lb_max','ub':'ub_max'})\n",
    "\n",
    "    print(\"Risk-Budgeting: μ, Σ input sensitivity (medians / max violations)\")\n",
    "    print(summary.round(6).to_string())\n",
    "    return df, summary, base\n",
    "\n",
    "df_rb, sum_rb, base_rb = rb_input_sensitivity(\n",
    "    mu_BL, Sigma_lw, budget=w_mkt,\n",
    "    R=5, mu_scale=0.01, sig_scale=0.01, seed=42, c=0.025\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb584e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1e6aa",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2f5f0",
   "metadata": {},
   "source": [
    "### Allocation comparison\n",
    "To compare allocation patterns across the different optimization methods, portfolio weights are displayed side by side in grouped bar charts. Each bar represents the normalized weight of an asset under a specific allocation scheme, including Equal Weight, Market Capitalisation, Mean–Variance, Maximum Sharpe ratio, and Risk Budgeting. This visualisation highlights similarities and differences in asset allocations, making it easier to identify concentration, diversification, and systematic tilts induced by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_weights.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336708a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = opt_weights.columns\n",
    "labels  = ['Equal Weight','Market Cap','Mean-Variance','Max Sharpe','Risk Budgeting']\n",
    "colors  = ['lightgray',   'orange',    \"#0f88ce\",   \"#13a05e\",   \"#e34646\"]\n",
    "\n",
    "x = np.arange(N)\n",
    "bar_width = 0.8 / len(cols_to_plot)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, (col, lab, colr) in enumerate(zip(cols_to_plot, labels, colors)):\n",
    "    ax.bar(\n",
    "        x + (i - (len(cols_to_plot)-1)/2) * bar_width,\n",
    "        opt_weights[col].values,\n",
    "        width  = bar_width,\n",
    "        label  = lab,\n",
    "        color  = colr,\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(opt_weights.index)\n",
    "for xi in x[:-1]:\n",
    "    ax.axvline(x=xi + 0.5, linestyle='--', linewidth=0.5,\n",
    "               color='gray', alpha=0.7)\n",
    "\n",
    "ax.set_xlim(-0.5, N - 0.5)\n",
    "ax.set_ylabel('Normalized weight')\n",
    "ax.set_title('Portfolio Allocations (Normalized)')\n",
    "ax.legend(ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4789bf",
   "metadata": {},
   "source": [
    "### Weight Tilts\n",
    "To better illustrate how each optimization method departs from the market portfolio, portfolio tilts are computed as the difference between the optimal weights and the corresponding market weights. These tilts are then displayed with diverging lollipop charts, where positive values indicate overweight relative to the market and negative values indicate underweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ab6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tilts_stacked(opt_weights, w_list, colors, methods, title=None):\n",
    "    \"\"\"Create a stacked lollipop chart of portfolio tilts vs market weights.\"\"\"\n",
    "    \n",
    "    tilts_df = (opt_weights[w_list].subtract(opt_weights['w_mkt'], axis=0))\n",
    "\n",
    "    tilts_df = tilts_df.iloc[::-1] # flip to have assets listed top→bottom\n",
    "    y = np.arange(len(tilts_df))\n",
    "    xlim = 1.10 * tilts_df.abs().to_numpy().max()\n",
    "\n",
    "    n = len(w_list)\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=1, figsize=(10, 3.2*n), sharex=True, sharey=True)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, col, color, name in zip(axes, w_list, colors, methods):\n",
    "        x1 = tilts_df[col].values\n",
    "        ax.hlines(y, 0.0, x1, linewidth=2, color=color)\n",
    "        ax.plot(x1, y, 'o', color=color, ms=5)\n",
    "        ax.axvline(0, linestyle='--', linewidth=1, color='black')\n",
    "\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(tilts_df.index, fontsize=8)\n",
    "\n",
    "        ax.grid(True, axis='x', linestyle=':', alpha=0.6)\n",
    "        ax.set_xlim(-xlim, xlim)\n",
    "        ax.set_title(name, loc='left', fontsize=11, pad=2)\n",
    "\n",
    "    axes[-1].set_xlabel('Tilt vs market ($w_{opt} - w_{mkt}$)', fontsize=10)\n",
    "    fig.suptitle(title, y=0.99, fontsize=12)\n",
    "\n",
    "    fig.tight_layout(pad=0.3)\n",
    "    fig.subplots_adjust(top=0.93)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "colors  = [\"#0f88ce\", \"#13a05e\", \"#e34646\"]\n",
    "w_list  = ['w_mv','w_ms','w_rb']\n",
    "methods = ['Mean-Variance','Max Sharpe','Risk Budgeting']\n",
    "\n",
    "plot_tilts_stacked(opt_weights, w_list, colors, methods,\n",
    "                   title='Tilts vs market')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522971b",
   "metadata": {},
   "source": [
    "### Risk Contribution\n",
    "To assess how portfolio risk is distributed across assets, risk contributions are computed as the product of portfolio weights and marginal contributions to risk, normalized by total portfolio variance. The resulting percentage contribution of each asset is displayed in bar charts for each allocation method. The dashed line indicates the equal-risk benchmark, enabling a direct comparison between the actual and the idealised equal-risk distribution. Annotated deviations highlight which assets contribute disproportionately to overall risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d332e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_contribution(weight, method):\n",
    "    \"\"\"Plot and report the risk contributions of a given portfolio.\"\"\"\n",
    "    \n",
    "    w = opt_weights[weight]\n",
    "    port_var    = w @ Sigma_lw @ w \n",
    "    marginal_rc = Sigma_lw @ w     \n",
    "    rc          = w * marginal_rc\n",
    "    pct_rc      = 100 * rc / port_var\n",
    "\n",
    "    rc_df = pd.DataFrame({\n",
    "        'Weight':               w,\n",
    "        'Marginal RC':          marginal_rc,\n",
    "        'Risk Contribution':    rc,\n",
    "        '% Risk Contribution':  pct_rc\n",
    "    }, index=weights_df.index).round(4)\n",
    "\n",
    "    equal_rc = 100/N\n",
    "    colors   = ['green' if x>equal_rc else 'red' for x in pct_rc]\n",
    "    tilt_rc  = pct_rc - equal_rc\n",
    "\n",
    "    x = np.arange(N)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bars = ax.bar(x, rc_df['% Risk Contribution'], color=colors)\n",
    "\n",
    "    # equal‐risk line\n",
    "    ax.axhline(equal_rc, linestyle='--', color='gray', linewidth=1,\n",
    "            label=f'Equal-risk ({equal_rc:.2f}%)')\n",
    "\n",
    "    for bar, d in zip(bars, tilt_rc):\n",
    "        y = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, \n",
    "                y + 0.5, \n",
    "                f\"{d:+.2f}%\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(rc_df.index, rotation=45, ha='right')\n",
    "\n",
    "    ax.set_ylabel('% Risk Contribution')\n",
    "    ax.set_title(f'Risk Contributions of {method} Portfolio')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for (weight, method) in zip(w_list, methods):\n",
    "    risk_contribution(weight, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fe48e",
   "metadata": {},
   "source": [
    "### Performance Statistics\n",
    "To compare the efficiency of the different allocation schemes, ex-ante performance measures are computed. For each portfolio, the following indicators are reported:\n",
    "- Expected return ($\\mu$), annualized and expressed in percent.\n",
    "- Variance ($\\sigma^2$), annualized and expressed in percent.\n",
    "- Volatility ($\\sigma$), as the square root of variance (annualized).\n",
    "- Sharpe ratio, defined as expected return per unit of volatility (annualized).\n",
    "- Diversification ratio (DR), computed as the weighted average of individual asset volatilities divided by the portfolio volatility.\n",
    "\n",
    "The resulting table allows a systematic comparison of the risk–return trade-off and diversification properties across all portfolio specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(weight_col, mu_BL, Sigma, opt_weights):\n",
    "    \"\"\"Compute performance statistics for given weight:\n",
    "    mean, variance, volatility and diversification ratio\"\"\"\n",
    "    w   = opt_weights[weight_col].values       \n",
    "    mu  = float(w @ mu_BL)\n",
    "    var = float(w @ Sigma @ w)\n",
    "    vol = np.sqrt(var)         \n",
    "\n",
    "    # Sharpe ratio \n",
    "    sharpe = mu / vol if vol > 0 else np.nan\n",
    "\n",
    "    # Diversification Ratio \n",
    "    sigma_assets = np.sqrt(np.diag(Sigma))         \n",
    "    dr = (w @ sigma_assets) / vol if vol > 0 else np.nan\n",
    "\n",
    "    return {\"mu [% p.a.]\": (mu * WEEKS_PER_YEAR) * 100, \n",
    "            \"var [% p.a.]\": (var * WEEKS_PER_YEAR) * 100, \n",
    "            \"vol (p.a.)\": (vol * np.sqrt(WEEKS_PER_YEAR)), \n",
    "            \"sharpe (p.a.)\": (sharpe * np.sqrt(WEEKS_PER_YEAR)), \n",
    "            \"DR\": dr\n",
    "            }\n",
    "\n",
    "w_list = opt_weights.columns\n",
    "\n",
    "results = {col: compute_stats(col, mu_BL, Sigma_lw, opt_weights) for col in w_list}\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ce3e5",
   "metadata": {},
   "source": [
    "### Portfolio Exposure\n",
    "To assess the sources of portfolio risk and return, each set of optimised weights is regressed on a set of factor returns (Fama–French five factors plus Momentum and $\\text{BAB}$). The regression is estimated with Newey–West heteroskedasticity and autocorrelation consistent (HAC) standard errors to account for weekly data.\n",
    "For each portfolio, the following outputs are reported:\n",
    "- $\\alpha$: the intercept of the regression, interpreted as abnormal return unexplained by the factors.\n",
    "- $\\beta$: the estimated coefficients on each factor, representing systematic sensitivities.\n",
    "- $R^2$: the proportion of portfolio variance explained by the factor model.\n",
    "\n",
    "This analysis provides a factor-based decomposition of the Mean–Variance, Maximum Sharpe, and Risk Budgeting portfolios, highlighting their tilts relative to well-established systematic risk premia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5835ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_exposures(w, R_assets, F_factors, nw_lags=4):\n",
    "    \"\"\"Estimate a portfolio's factor exposures via OLS with Newey-West (HAC) errors.\n",
    "    Output: alpha, beta, r2.\"\"\"\n",
    "\n",
    "    R, F = R_assets.align(F_factors, join=\"inner\", axis=0)\n",
    "    r_p  = R.values @ w\n",
    "    X    = sm.add_constant(F.values)\n",
    "    ols  = sm.OLS(r_p, X).fit()\n",
    "    nw   = ols.get_robustcov_results(cov_type=\"HAC\", maxlags=nw_lags)\n",
    "    alpha = nw.params[0]\n",
    "    beta = pd.Series(nw.params[1:], index=F.columns)\n",
    "    return alpha, beta, ols.rsquared\n",
    "\n",
    "fcols = [\"MKT\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"MOM\",\"BAB\"] \n",
    "F = factors_exc[fcols].dropna()\n",
    "\n",
    "alpha_mv, beta_mv, r2_mv = portfolio_exposures(opt_weights['w_mv'], asset_universe_exc, F)\n",
    "alpha_ms, beta_ms, r2_ms = portfolio_exposures(opt_weights['w_ms'], asset_universe_exc, F)\n",
    "alpha_rb, beta_rb, r2_rb = portfolio_exposures(opt_weights['w_rb'], asset_universe_exc, F)\n",
    "\n",
    "\n",
    "results = {\n",
    "    'MV': {'alpha (% p.a.)': 100*alpha_mv*WEEKS_PER_YEAR,\n",
    "           'R2': r2_mv, **beta_mv.to_dict()},\n",
    "    'MS': {'alpha (% p.a.)': 100*alpha_ms*WEEKS_PER_YEAR,\n",
    "           'R2': r2_ms, **beta_ms.to_dict()},\n",
    "    'RB': {'alpha (% p.a.)': 100*alpha_rb*WEEKS_PER_YEAR,\n",
    "           'R2': r2_rb, **beta_rb.to_dict()},\n",
    "}\n",
    "\n",
    "df_exposures = pd.DataFrame(results).T  \n",
    "df_exposures.index.name = 'Portfolio'\n",
    "df_exposures = df_exposures[['alpha (% p.a.)'] + fcols + ['R2']]\n",
    "\n",
    "print(df_exposures.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_lab (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
